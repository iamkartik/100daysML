{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning with Pytorch(NLP).ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "VQ83LXgRWRQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "b3dcbd21-dfbe-4f3c-a16c-b672eda84ecb"
      },
      "cell_type": "code",
      "source": [
        "! pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 31kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x59076000 @  0x7fc9923e91c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rFjGL5eehDuK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Building Blocks - Affine Maps , Non-Linearities  and Objectives\n",
        "\n",
        "Deep Learning - Composing Linear and Non-Linearities "
      ]
    },
    {
      "metadata": {
        "id": "QnAXU1vxhiP1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Affine Maps\n",
        "\n",
        "Core workhorses of deep learning . It is a function $f(x)$ $$ f(x) = A(x) +b $$ \n",
        "\n",
        "where for a matrix A and vectors x,b. The parameters to be learned are 'A' and 'b' , Often b is refered as bias term\n",
        "\n",
        "Map the rows of input instead of the columns => i'th row of output <-> maps to <-> i'th row of input under A + bias term"
      ]
    },
    {
      "metadata": {
        "id": "yX4aWSBtjRsS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_p-UY2PijeIc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e06bf825-7f63-42df-ae4d-34b1cfff171d"
      },
      "cell_type": "code",
      "source": [
        "# To ensure the random numbers generated are same over time , seeding manually\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f153c0a6090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "1r7jEy4Xj8xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6cbe04a9-223a-40f6-dac4-fbd1df3f1980"
      },
      "cell_type": "code",
      "source": [
        "# linear layer which maps from R^5 -> R^3 \n",
        "# contains parameters A and b \n",
        "lin = nn.Linear(5,3)\n",
        "# input data\n",
        "data = torch.randn(2,5)\n",
        "\n",
        "print(lin(data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.1755, -0.3268, -0.5069],\n",
            "        [-0.6602,  0.2260,  0.1089]], grad_fn=<ThAddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ax2ll98MkdmU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Non-Linearities\n",
        "\n",
        "Suppose there are two affine maps $f(x)=Ax+b$ and $g(x) = Cx+d$  find $f(g(x))$ \n",
        "\n",
        "$$ f(g(x))=A(Cx+d)+b = ACx+(Ad+b)$$\n",
        "\n",
        "AC is a matrix and Ad+b is a vector => **composing affine maps gives affine maps **\n",
        "\n",
        "Thus if we want a neural network to be long chains of affine compositions, There is **no new power added to the network** ie the final output can also be achieved by doing a single affine map. \n",
        "\n",
        "'without a non-linear activation function in the network, a NN, no matter how many layers it had, would behave just like a single-layer perceptron, because summing these layers would give you just another linear function'\n",
        "\n",
        "**By Introducing non-linearities between the affine layers , we can build much more powerful models **\n",
        "\n",
        "Some common non-linearites include $tanh(x) , \\sigma(x), ReLU(x)$ as their gradients are easy to compute  eg. \n",
        "\n",
        "$$\\frac {d\\sigma}{dx} = \\sigma(x)(1-\\sigma(x))$$\n",
        "\n",
        "*Note*-  **$\\sigma(x)$ gradient vanishes very quickly** use **tanh or ReLU instead**\n"
      ]
    },
    {
      "metadata": {
        "id": "_JvjCAVIsfgh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "567d0104-8c75-4c39-fcb5-ddf752f838b0"
      },
      "cell_type": "code",
      "source": [
        "# In pytorch most nonLinearitieas are in  torch.functional \n",
        "data = torch.randn(2,2)\n",
        "print(data)\n",
        "print(F.relu(data))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5404, -2.2102],\n",
            "        [ 2.1130, -0.0040]])\n",
            "tensor([[0.0000, 0.0000],\n",
            "        [2.1130, 0.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rv0OZ_aYsy5N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Softmax and Probabilities\n",
        "\n",
        "** $Softmax(x)$ ** also a non-linearity usually used as the last operation done in the network . **Takes a vector of real numbers and returns a probability distribution **  Let x be a vector of real numbers (-/+) , then i'th component of $Softmax(x)$ is \n",
        "\n",
        "$$ \\frac {\\exp(x_i)} {\\sum_j\\exp(x_j) }$$\n",
        "\n",
        "The output is probability distribution - all elements >=0, summing to 1\n"
      ]
    },
    {
      "metadata": {
        "id": "gKl5Nhp8s6Ah",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e734a841-c572-49fc-d012-3ce3d3152647"
      },
      "cell_type": "code",
      "source": [
        "# Softmax also in nn.functional\n",
        "\n",
        "data = torch.randn(5)\n",
        "print(data)\n",
        "print(F.softmax(data,dim=0))\n",
        "# sum =1\n",
        "print(F.softmax(data,dim=0).sum())\n",
        "# log softmax - log(softmax) ie log(exp_i/exp(x).sum())\n",
        "print(F.log_softmax(data,dim=0))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2.2820, -1.2080,  1.1120,  2.2174, -0.4269])\n",
            "tensor([0.4264, 0.0130, 0.1324, 0.3998, 0.0284])\n",
            "tensor(1.)\n",
            "tensor([-0.8523, -4.3423, -2.0222, -0.9168, -3.5611])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9FCNFnJHvF3N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Objective Function \n",
        "\n",
        "The function that the network is being trained to minimize ( Loss function/ Cost function )\n",
        "\n",
        "Training instance --> Neural Network --> Loss of output --> update model by taking derivative of loss \n",
        "\n",
        "Eg **Negetive Log Likelihood Loss ** - multi class classification . Train  the network to minimize the negetive log probability of the correct output ( or maximize the log probability of correct output)"
      ]
    },
    {
      "metadata": {
        "id": "Eqi-GFEE11CB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimization and Training\n",
        "\n",
        "With **requires_grad = True** , a Tensor remembers the operation used to create it , eg - z = x+y , then z.grad_fn contains the info that it was a sum of x&y.\n",
        "Using this info the tensor can compute gradients w.r.t the things that were used to compute it.\n",
        "\n",
        "Final Loss - after loss function is also a tensor , we can compute gradients w.r.t all parameters used to compute it .And then can perform standard gradient update\n",
        "Let $\\theta$ be the parameters ,$L(\\theta)$ be the loss function and $\\eta$ learning rate then :\n",
        "\n",
        "$$ \\theta^{(t+1)} = \\theta^{(t)}  - \\eta\\nabla_\\theta L(\\theta)$$\n",
        "\n",
        "Where $\\nabla$ (Del or nabla ) is a vector diffenrtial operator\n",
        "\n",
        "Torch has torch.optim package that can handle all the calculations . Optimization - different learning ratesm different update algo's like replacing SGD with Adam or RMSProp can lead to better performance.\n"
      ]
    },
    {
      "metadata": {
        "id": "8HmZFF1p6n9J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Creating Network Components in Pytorch\n",
        "\n",
        "Creating a network that takes in a sparse bag-of-words represenations and outputs a probability distribution over two labels \"English\" and \"Spanish\" (Logistic Regression)\n",
        "\n",
        "** Logistic Bag-Of-words Classifier **\n",
        "\n",
        "map a sparse BOW representation to log probabilities over labels . Assign each word in vocab an index . Eg only two  words - 'Hello' and 'World' -> assign index 0,1\n",
        "\n",
        "Thus Hello -> [1,0]  , Hello Hello  World -> [2,1] \n",
        "\n",
        "In general  $$ [Count(Hello),Count(World)]$$\n",
        "\n",
        "Denote this BOW vector as x , then output of the network is :\n",
        "$$ log Softmax(Ax+b)$$\n"
      ]
    },
    {
      "metadata": {
        "id": "QM-YV9LZ6utJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# data =[(\"me gusta comer en la cafeteria\".split(),\"Spanish\"),\n",
        "#       (\"Give me the location\".split(),\"English\"),\n",
        "#       (\"Donde esta la biblioteca\".split(),\"Spanish\"),\n",
        "#       (\"Where is the train station\".split(),\"English\")]\n",
        "\n",
        "# test_data = [(\"Yo creo que si\".split(),\"Spanish\"),\n",
        "#             (\"I cannot find the bottle\".split(),\"English\")]\n",
        "\n",
        "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
        "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
        "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
        "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
        "\n",
        "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
        "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l_MpsFu2Du32",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b6cfc2f9-d293-496b-92c8-719ce996f467"
      },
      "cell_type": "code",
      "source": [
        "# Word to index mapping , map each word in vocab to a unique integer\n",
        "# which will be its index into the bag-of-words vector\n",
        "\n",
        "word_to_ix={}\n",
        "\n",
        "for sentence, _ in data + test_data:\n",
        "  for word in sentence:\n",
        "    if word not in word_to_ix:\n",
        "      # word:index (current length)\n",
        "      word_to_ix[word] = len(word_to_ix)\n",
        "      \n",
        "print(word_to_ix)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J8SvrQnAEdaH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(word_to_ix)\n",
        "NUM_LABELS =2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXwOOvoNEn3Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The neural network to classify the text \n",
        "\n",
        "class BOWClassifier(nn.Module):\n",
        "  \n",
        "  def __init__(self,num_labels,vocab_size):\n",
        "    \n",
        "    super(BOWClassifier,self).__init__()\n",
        "    # Defining the linear layer that takes an input of size vocab size \n",
        "    # and outputs maps to the number of labels\n",
        "    # Need the parameters A and b of affine mapping\n",
        "    # Using nn.Linear that provides the affine map\n",
        "    self.linear = nn.Linear(vocab_size,num_labels)\n",
        "    \n",
        "  def forward(self,bow_vec):\n",
        "    # Pass the input through the linear layer \n",
        "    # apply log softmax at the end\n",
        "    return F.log_softmax(self.linear(bow_vec),dim=1)\n",
        "  \n",
        "  \n",
        "#Helper functions to map input and output\n",
        "  \n",
        "def make_bow_vector(sentence,word_to_ix):\n",
        "    vec = torch.zeros(len(word_to_ix))\n",
        "    for word in sentence:\n",
        "      vec[word_to_ix[word]] += 1\n",
        "    return vec.view(1,-1)\n",
        "  \n",
        "def make_target(label,label_to_ix):\n",
        "    return torch.LongTensor([label_to_ix[label]])\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vTobevzVGuB1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = BOWClassifier(NUM_LABELS,VOCAB_SIZE)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vwBdnH1rL6Zp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2c126eeb-ccaa-494e-c1ba-1b47ef6ff2d6"
      },
      "cell_type": "code",
      "source": [
        "# First param is A , second is b\n",
        "for param in model.parameters():\n",
        "  print(param)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.1130,  0.1821, -0.1218,  0.0426,  0.1692,  0.1300,  0.1222,  0.1394,\n",
            "          0.1240,  0.0507, -0.1341, -0.1647, -0.0899, -0.0228, -0.1202,  0.0717,\n",
            "          0.0607, -0.0444,  0.0754,  0.0634,  0.1197,  0.1321, -0.0664,  0.1916,\n",
            "         -0.0227, -0.0067],\n",
            "        [-0.1851, -0.1262, -0.1146, -0.0839,  0.1394, -0.0641, -0.1466,  0.0755,\n",
            "          0.0628,  0.1270, -0.1015,  0.0425, -0.0714, -0.0441, -0.1563, -0.0894,\n",
            "         -0.0601,  0.0839,  0.0358,  0.0484,  0.1957,  0.1911,  0.1338,  0.0062,\n",
            "         -0.1357,  0.1533]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([-0.0490, -0.0159], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZBKru3PqOCiU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0a585e98-1b82-4f74-8aa1-737101472f42"
      },
      "cell_type": "code",
      "source": [
        "# Running a sample run thus no grad\n",
        "with torch.no_grad():\n",
        "  sample = data[0]\n",
        "  print(sample)\n",
        "  bow_vector = make_bow_vector(sample[0],word_to_ix)\n",
        "  print(bow_vector)\n",
        "  log_probs = model(bow_vector)\n",
        "  print(log_probs)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['me', 'gusta', 'comer', 'en', 'la', 'cafeteria'], 'SPANISH')\n",
            "tensor([[1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[-0.3365, -1.2527]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "egCT1Tq8OtaN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Training the model\n",
        "\n",
        "pass instances through model to get log_probabilites -> compute loss -> compute gradient of loss -> update params with gradient step\n",
        "\n",
        "Loss function - nn.NLLLoss() , optimizer - optim.SGD\n",
        "\n",
        "input to NLLLoss is a vector of log_probabilities and a target label , it doesn't compute the log probabilities . \n",
        "In case of 'nn.CrossEntropyLoss()' ( similar to NLLLoss) the log_softmax step is done "
      ]
    },
    {
      "metadata": {
        "id": "ive4k4axPv5w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "354b2860-5b3b-4924-eea7-f80d5f6fc022"
      },
      "cell_type": "code",
      "source": [
        "# label indexes\n",
        "label_to_ix ={\"SPANISH\":0,\"ENGLISH\":0}\n",
        "\n",
        "# Running on test data to compare before and after \n",
        "with  torch.no_grad():\n",
        "  print('before training')\n",
        "  for statement, label in test_data:\n",
        "    bow_vec = make_bow_vector(statement,word_to_ix)\n",
        "    log_probs = model(bow_vec)\n",
        "    print(log_probs)\n",
        "\n",
        "# printing the matrix column corresponding to \"creo\"\n",
        "print('Accuracy ->',next(model.parameters())[:,word_to_ix[\"creo\"]])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before training\n",
            "tensor([[-0.6806, -0.7059]])\n",
            "tensor([[-0.5845, -0.8150]])\n",
            "Accuracy -> tensor([-0.1341, -0.1015], grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cYmmMPJTQ9Kl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bec15894-46d8-499f-a2a2-56a0017f4e03"
      },
      "cell_type": "code",
      "source": [
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.1)\n",
        "\n",
        "# large epoch as small dataset\n",
        "for epoch in range(100):\n",
        "  for statement,label in data:\n",
        "    # Need to clear the gradients as they get accumulated\n",
        "    model.zero_grad()\n",
        "    \n",
        "    # Making the input and output as tensors\n",
        "    bow_vec = make_bow_vector(statement,word_to_ix)\n",
        "    target = make_target(label,label_to_ix)\n",
        "    \n",
        "    # Forward Pass\n",
        "    log_probs = model(bow_vector)\n",
        "    \n",
        "    # Calculate loss , gradient and backpropagate error , update params\n",
        "    loss = loss_function(log_probs,target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "# after training running the test set\n",
        "with torch.no_grad():\n",
        "  print('After training')\n",
        "  for statement,label in test_data:\n",
        "    bow_vec = make_bow_vector(statement,word_to_ix)\n",
        "    log_probs = model(bow_vec)\n",
        "    print(log_probs)\n",
        "    \n",
        "print('Accuracy ->',next(model.parameters())[:,word_to_ix[\"creo\"]])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After training\n",
            "tensor([[-0.3715, -1.1702]])\n",
            "tensor([[-0.1563, -1.9333]])\n",
            "Accuracy -> tensor([-0.1341, -0.1015], grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FHkHMB2CSwAB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}