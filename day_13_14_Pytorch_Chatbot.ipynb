{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch Chatbot.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OOrRnf5HOQ8I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ChatBot Tutorial \n",
        "\n",
        "from https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n",
        "\n",
        "Using Recurrent seq-toseq modeling train a simple chatbot on Cornell Movie Dialogue Corpus\n",
        "\n",
        "Objective : Learn seq to seq modeling , RNN ,training encoder-decoder together, get more practice on text data and NLP tasks\n",
        "\n",
        "Future: Use the same model on some different dataset"
      ]
    },
    {
      "metadata": {
        "id": "SXWLyMO0O14p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### StartupTasks"
      ]
    },
    {
      "metadata": {
        "id": "cThGka-gyvi8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "9c2425a4-d204-4cd9-c468-d918367a4aa7"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 27kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5856c000 @  0x7f500738a2a4 0x594e17 0x626104 0x51190a 0x4f5277 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x510c78 0x5119bd 0x4f5277 0x4f3338 0x510fb0 0x5119bd 0x4f6070 0x4f3338 0x510fb0 0x5119bd 0x4f6070\n",
            "\u001b[?25hCollecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 19.6MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Installing collected packages: torch, pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torch-0.4.1 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "67ogHOVoyx55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "dd5c3f4a-5126-4a8e-a43f-28228b55fd9a"
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-22 11:53:52--  http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.20\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.20|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9916637 (9.5M) [application/zip]\n",
            "Saving to: ‘cornell_movie_dialogs_corpus.zip’\n",
            "\n",
            "cornell_movie_dialo 100%[===================>]   9.46M  5.91MB/s    in 1.6s    \n",
            "\n",
            "2018-10-22 11:53:53 (5.91 MB/s) - ‘cornell_movie_dialogs_corpus.zip’ saved [9916637/9916637]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OPFSGi_OP9pN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af2640b8-a194-492c-efd3-ac29630ef600"
      },
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!mv /content/cornell_movie_dialogs_corpus.zip /content/data/\n",
        "!unzip -q /content/data/cornell_movie_dialogs_corpus.zip\n",
        "!ls /content/data/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cornell_movie_dialogs_corpus.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vZ-hDNyAQYwN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b6f27a50-8bfc-4788-8470-428b0dc88508"
      },
      "cell_type": "code",
      "source": [
        "!ls /content/'cornell movie-dialogs corpus'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chameleons.pdf\t\t       movie_lines.txt\t\t  README.txt\n",
            "movie_characters_metadata.txt  movie_titles_metadata.txt\n",
            "movie_conversations.txt        raw_script_urls.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vPS7DceKy7gd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.jit import script,trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VjDbARhzy-5D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fkjtVirPzglU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading and Preprocessing data\n",
        "\n",
        "~ 250 lines of preprocessing code"
      ]
    },
    {
      "metadata": {
        "id": "GHRjr_2DzhbJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "20b2bcfa-2ddd-4c86-efa0-ff10c07bc218"
      },
      "cell_type": "code",
      "source": [
        "# Looking at the data \n",
        "\n",
        "corpus_name = \"cornell movie-dialogs corpus\"\n",
        "corpus = os.path.join(corpus_name)\n",
        "\n",
        "def printlines(file,n=10):\n",
        "  with open(file,\"rb\") as datafile:\n",
        "    lines = datafile.readlines()\n",
        "  \n",
        "  for line in lines[:n]:\n",
        "    print(line)\n",
        "    \n",
        "printlines(os.path.join(corpus,\"movie_lines.txt\"))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
            "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
            "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
            "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
            "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
            "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
            "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
            "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
            "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
            "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4ZXm4e7Azr0b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create Formatted Data File** \n",
        "\n",
        "Tab separated query and response sentences\n",
        "\n",
        "loadLines - split each line of the file into a dictionary of fields (lineID,characterId,movieId,character,text)\n",
        "\n",
        "loadConversations - group fields of lines from loadLines into conversations based on movie_conversation.txt\n",
        "\n",
        "extractSentencePairs extracts pars of sentences from conversations "
      ]
    },
    {
      "metadata": {
        "id": "sJkNNef3zuE6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split each line of the file into a dictionary of fields\n",
        "def loadLines(filename,fields):\n",
        "  lines ={}\n",
        "  with open(filename,'r',encoding='iso-8859-1') as f:\n",
        "    for line in f:\n",
        "      values = line.split(\"+++$+++\")\n",
        "      # extracted fields\n",
        "      line_obj={}\n",
        "      for i,field in enumerate(fields):\n",
        "        # object\n",
        "        line_obj[field]=values[i]\n",
        "      # line id key for nested line object \n",
        "      # stripping as lineID is coming with a space in end 'L194 '\n",
        "      lines[line_obj['lineID'].strip()]=line_obj\n",
        "  return lines\n",
        "\n",
        "# group fields of lines from loadlines into conversations based on movie_conversations\n",
        "def loadConversations(filename,lines,fields):\n",
        "  conversations=[]\n",
        "  with open(filename,'r',encoding='iso-8859-1') as f:\n",
        "    for line in f:\n",
        "      values = line.split(\"+++$+++\")\n",
        "      conv_obj={}\n",
        "      for i,field in enumerate(fields):\n",
        "        conv_obj[field] = values[i]\n",
        "      #Convert string to a list conv_obj[\"utteranceID\"] ==\"[L598485,L...]\"\n",
        "      lineIds = eval(conv_obj[\"utteranceID\"])\n",
        "      # reassemble lines\n",
        "      conv_obj[\"lines\"]=[]\n",
        "      for lineId in lineIds:\n",
        "        conv_obj[\"lines\"].append(lines[lineId])\n",
        "      conversations.append(conv_obj)\n",
        "  return conversations\n",
        "\n",
        "# Extract pair of sentences from conversations\n",
        "def extractSentencePairs(conversations):\n",
        "  qa_pairs=[]\n",
        "  for conversation in conversations:\n",
        "    # Iterate over all lines \n",
        "    for i in range(len(conversation[\"lines\"])-1):\n",
        "      inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
        "      targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "      # add to qa pair if both exist\n",
        "      if inputLine and targetLine:\n",
        "        qa_pairs.append([inputLine,targetLine])\n",
        "  return qa_pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aINj5aT5z6t-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4069cf9f-6d7c-410e-ca42-64f0b279b37a"
      },
      "cell_type": "code",
      "source": [
        "# using the functions to generate conversation and create a new file - datafile\n",
        "\n",
        "datafile = os.path.join(corpus,\"formatted_movie_lines.txt\")\n",
        "\n",
        "delimiter ='\\t'\n",
        "# unescaping the delimiter\n",
        "delimiter = str(codecs.decode(delimiter,\"unicode_escape\"))\n",
        "\n",
        "\n",
        "#Initialize the lines dict , conversation dict and fields for lines and conversation\n",
        "lines={}\n",
        "conversations=[]\n",
        "MOVIE_LINES_FIELDS = [\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\n",
        "MOVIE_CONVERSATION_FIELDS = [\"characterID\",\"character2ID\",\"movieID\",\"utteranceID\"]\n",
        "\n",
        "# Load lines and preproces conversations \n",
        "print(\"Preprocessing, loading lines\")\n",
        "lines = loadLines(os.path.join(corpus,\"movie_lines.txt\"),MOVIE_LINES_FIELDS)\n",
        "print(\"Loading Conversations\")\n",
        "conversations = loadConversations(os.path.join(corpus,\"movie_conversations.txt\"),lines,MOVIE_CONVERSATION_FIELDS)\n",
        "\n",
        "# writing a new file\n",
        "print(\"writing formatted file\")\n",
        "with open(datafile,'w',encoding='utf-8') as outfile:\n",
        "  writer = csv.writer(outfile,delimiter=delimiter)\n",
        "  for pair in extractSentencePairs(conversations):\n",
        "    writer.writerow(pair)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing, loading lines\n",
            "Loading Conversations\n",
            "writing formatted file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HJy7AiyJ0fKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "c503ba66-c44d-4130-c9c7-1c96aff927be"
      },
      "cell_type": "code",
      "source": [
        "#printing sample data from formatted file \n",
        "printlines(datafile)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\r\\n\"\n",
            "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\n\"\n",
            "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\n\"\n",
            "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\n\"\n",
            "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\n\"\n",
            "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\r\\n\"\n",
            "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\r\\n\"\n",
            "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\r\\n'\n",
            "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\r\\n\"\n",
            "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\r\\n'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zRSPm2Aa0jf1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Load and Trim data **\n",
        "\n",
        "Creating a vocabulary and loading query/response pairs\n"
      ]
    },
    {
      "metadata": {
        "id": "ZiA3UmAS0qgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# default word tokens\n",
        "PAD_token = 0 # for padding short sentences\n",
        "SOS_token = 1 # start of sentence token\n",
        "EOS_token = 2 # end of sentence token \n",
        "\n",
        "class Voc:\n",
        "  def __init__(self,name):\n",
        "    self.name = name\n",
        "    self.trimmed = False\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {PAD_token:\"PAD\",SOS_token:\"SOS\",EOS_token:\"EOS\"}\n",
        "    self.num_words = 3 # default 3 \n",
        "  \n",
        "  \n",
        "  def addSentence(self,sentence):\n",
        "    for word in sentence.split(' '):\n",
        "      self.addWord(word)\n",
        " \n",
        "  def addWord(self,word):\n",
        "    if word not in self.word2index:\n",
        "      self.word2index[word] = self.num_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.num_words] = word\n",
        "      self.num_words += 1\n",
        "    else:\n",
        "      self.word2count[word] +=1\n",
        " #remove words below a certain threshold\n",
        "  def trim(self,min_count):\n",
        "      if self.trimmed:\n",
        "        return\n",
        "      self.trimmed = True\n",
        "      keep_words = []\n",
        "      for k,v in self.word2count.items():\n",
        "        if v>=min_count:\n",
        "          keep_words.append(k)\n",
        "    \n",
        "      print(f'Keep words {len(keep_words)},{len(self.word2index)},{len(keep_words)/len(self.word2index)}')\n",
        "    \n",
        "      # reinitialize dictionaries\n",
        "      self.word2index = {}\n",
        "      self.word2count = {}\n",
        "      self.index2words = {PAD_token:\"PAD\",SOS_token:\"SOS\",EOS_token:\"EOS\"}\n",
        "      self.num_words = 3 # default 3 tokens\n",
        "    \n",
        "      for word in keep_words:\n",
        "        self.addWord(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S1hR5u9OkGzl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " Assemble vocabulary and query/response pairs ,\n",
        " \n",
        " But need to convert Unicode strings to ASCII using unicodeToAscii \n",
        " \n",
        "Then convert all characters to lowercase and trim non-letter characters - normalizeString\n",
        "\n",
        "Finally filter out sentences with length greater than MAX_LENGTH - filterPairs"
      ]
    },
    {
      "metadata": {
        "id": "mTuDzajVlSl5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "d7236ec0-9479-420c-c147-975e8160651a"
      },
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10 # max sentence length to consider\n",
        "\n",
        "# turning unicode to Ascii\n",
        "def unicodeToAscii(s):\n",
        "  #\"Mn\" stands for Nonspacing_Mark\n",
        "  return ''.join(\n",
        "    c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c)!='Mn')\n",
        "\n",
        "# lowercase and rim and remove non-letter character\n",
        "def normalizeString(s):\n",
        "  s = unicodeToAscii(s.lower().strip())\n",
        "  s = re.sub(r\"([.!?])\",r\" \\1\",s)\n",
        "  s = re.sub(r\"[^a-zA-Z.!?]+\",r\" \",s)\n",
        "  s = re.sub(r\"\\s+\",r\" \",s).strip()\n",
        "  return s\n",
        "\n",
        "# read a query response pair and return a vocab object\n",
        "def readVocs(datafile,corpus_name):\n",
        "  print(\"reading lines\")\n",
        "  # read file and split into lines\n",
        "  lines = open(datafile,encoding='utf-8').read().strip().split('\\n')\n",
        "  # split every line into pairs and normalize\n",
        "  pairs =[[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "  voc = Voc(corpus_name)\n",
        "  return voc,pairs\n",
        "\n",
        "# Return true if both sentences in pair are under max length\n",
        "def filterPair(p):\n",
        "  return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "# check which pairs are valid\n",
        "def filterPairs(pairs):\n",
        "  return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "# Using all of the above method return a populated voc object and pairs list\n",
        "def loadPrepareData(corpus,corpus_name,datafile,save_dir):\n",
        "  print('Start preparing training data')\n",
        "  voc,pairs = readVocs(datafile,corpus_name)\n",
        "  print(f\"Read {len(pairs)} sentence pairs\")\n",
        "  pairs = filterPairs(pairs)\n",
        "  print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
        "  for pair in pairs:\n",
        "    voc.addSentence(pair[0])\n",
        "    voc.addSentence(pair[1])\n",
        "  print(\"Counted Words :\",voc.num_words)\n",
        "  return voc,pairs\n",
        "\n",
        "# Load/Assemble voc and pairs\n",
        "save_dir = os.path.join(\"data\",\"save\")\n",
        "voc,pairs = loadPrepareData(corpus,corpus_name,datafile,save_dir)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start preparing training data\n",
            "reading lines\n",
            "Read 221282 sentence pairs\n",
            "Trimmed to 64271 sentence pairs\n",
            "Counted Words : 18008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b7Wz2cIK1XRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7fbc3fd6-3b20-4985-e49d-be49202e07b8"
      },
      "cell_type": "code",
      "source": [
        "# checking some pairs\n",
        "for pair in pairs[:10]:\n",
        "  print(pair)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['there .', 'where ?']\n",
            "['you have my word . as a gentleman', 'you re sweet .']\n",
            "['hi .', 'looks like things worked out tonight huh ?']\n",
            "['you know chastity ?', 'i believe we share an art instructor']\n",
            "['have fun tonight ?', 'tons']\n",
            "['well no . . .', 'then that s all you had to say .']\n",
            "['then that s all you had to say .', 'but']\n",
            "['but', 'you always been this selfish ?']\n",
            "['do you listen to this crap ?', 'what crap ?']\n",
            "['what good stuff ?', 'the real you .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gI2ZuX7o2DRK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Trimming rarely used words for faster convergence during training \n",
        "\n",
        "Trim words used under MIN_COUNT threshold using voc.trim\n",
        "\n",
        "Filter out pairs with trimmed words"
      ]
    },
    {
      "metadata": {
        "id": "LzKpyeg22jLF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MIN_COUNT = 3\n",
        "\n",
        "def trimRareWords(voc,pairs,MIN_COUNT):\n",
        "  # trim words used under the MIN_COUNT from voc\n",
        "  voc.trim(MIN_COUNT)\n",
        "  # filter out pairs with trimmed words\n",
        "  keep_pairs=[]\n",
        "  for pair in pairs:\n",
        "    input_sentence = pair[0]\n",
        "    output_sentence = pair[1]\n",
        "    keep_input = True\n",
        "    keep_output = True\n",
        "    # check input sentence \n",
        "    for word in input_sentence.split(' '):\n",
        "      if word not in voc.word2index:\n",
        "        keep_input = False\n",
        "        break\n",
        "    # check output sentence \n",
        "    for word in output_sentence.split(' '):\n",
        "      if word not in voc.word2index:\n",
        "        keep_output = False\n",
        "        break\n",
        "    # keep pair if both input and output contain words greater than min frequency\n",
        "    if keep_input and keep_output:\n",
        "      keep_pairs.append(pair)\n",
        "  print(f\"Trimmed from {len(pairs)} to {len(keep_pairs)} , {len(keep_pairs)/len(pairs)} of total\")\n",
        "  return keep_pairs    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hhbj9o1B4DCg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ef70017e-8e08-4c44-82f0-71564dbea873"
      },
      "cell_type": "code",
      "source": [
        "# Trim voc and pairs\n",
        "pairs = trimRareWords(voc,pairs,MIN_COUNT)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keep words 7823,18005,0.43449041932796445\n",
            "Trimmed from 64271 to 53165 , 0.8272004481025657 of total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iuWVfmoB4LSe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Prepare Data for Model **\n",
        "\n",
        "Need to convert words into numerical torch tensors . \n",
        "\n",
        "Using mini-batch allows for faster training . But will have to accomodate sentences of different sizes in the same batch. Thus will make the batched input tensor of shape ( max_length , batch_size). Sentences shorter than max_length are zero padded after EOS_token\n",
        "\n",
        "\n",
        "Simply converting sentences to their indexes ( indexesFromSentences ) and zero pad , the resulting tensor will have shape (batch_size,max_length)  and indexing the first dimension will return a full sequence across all time steps. - \n",
        "\n",
        "But we need to index the batch across time and across all sequences in the batch , Thus will have to take transpose of the input batch to ( max_length, batch_size) . This ensures that indexing along first dimension retursn a time step across all sentences in the batch\n",
        "\n",
        "Convert sentences to tensor and return a tensor of lengths for all sequences in batch ( to be passed to decoder ) - inputVar\n",
        "\n",
        "outputVar - similar to inputVar but instead of returning a lengths tensor , return a binary mask tensor and max target sentence length . Binary mask tensor has the same shape as the output target tensor but every element that is PAD_token is 0 and rest are 1\n",
        "\n",
        "batch2TrainData - take a bunch of  pairs and return the inpit and target tensor \n"
      ]
    },
    {
      "metadata": {
        "id": "pQCLlnSrKR-8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(voc,sentence):\n",
        "  return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "  return list(itertools.zip_longest( *l,fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "  m =[]\n",
        "  for i, seq in enumerate(l):\n",
        "    m.append([])\n",
        "    for token in seq:\n",
        "      if token == PAD_token:\n",
        "        m[i].append(0)\n",
        "      else:\n",
        "        m[i].append(1)\n",
        "  return m\n",
        "\n",
        "\n",
        "# returns padded input sequence tensor amd lengths\n",
        "def inputVar(l,voc):\n",
        "  indexes_batch =  [indexesFromSentence(voc,sentence) for sentence in l]\n",
        "  lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "  padList = zeroPadding(indexes_batch)\n",
        "  padVar = torch.LongTensor(padList)\n",
        "  return padVar,lengths\n",
        "\n",
        "\n",
        "# return padded target sequence tensor, padding mask and max target length\n",
        "def outputVar(l,voc):\n",
        "  indexes_batch = [indexesFromSentence(voc,sentence) for sentence in l]\n",
        "  max_target_len = max([len(indexes) for indexes in  indexes_batch])\n",
        "  padList = zeroPadding(indexes_batch)\n",
        "  mask = binaryMatrix(padList)\n",
        "  mask = torch.ByteTensor(mask)\n",
        "  padVar = torch.LongTensor(padList)\n",
        "  return padVar,mask,max_target_len\n",
        "\n",
        "# return all items for a given batch of pairs\n",
        "def batch2TrainData(voc,pair_batch):\n",
        "  pair_batch.sort(key=lambda x:len(x[0].split(\" \")),reverse=True )\n",
        "  input_batch, output_batch =[],[]\n",
        "  \n",
        "  for pair in pair_batch:\n",
        "    input_batch.append(pair[0])\n",
        "    output_batch.append(pair[1])\n",
        "  inp,lengths = inputVar(input_batch,voc)\n",
        "  output,mask,max_target_len =  outputVar(output_batch,voc)\n",
        "  return inp,lengths,output,mask,max_target_len\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q7Lp7vqFNxD5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "afafe775-feb1-420c-8bde-45260a73b0be"
      },
      "cell_type": "code",
      "source": [
        "# Validating\n",
        "small_batch_size =5\n",
        "batches = batch2TrainData(voc,[random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable,lengths,target_variable,mask, max_target_len = batches\n",
        "\n",
        "print(\"input variables \",input_variable)\n",
        "print(\"lengths \",lengths)\n",
        "print(\"target variable \", target_variable)\n",
        "print(\"mask \",mask)\n",
        "print(\"max target len\",max_target_len)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input variables  tensor([[ 167,   38,   65,   25,   16],\n",
            "        [ 101,  266,  331,  197, 1000],\n",
            "        [  37,   59,  117,  117,    4],\n",
            "        [3597,   76,  401,    4,    2],\n",
            "        [ 230,   60, 1735,    2,    0],\n",
            "        [  52,    4,    4,    0,    0],\n",
            "        [  18,    2,    2,    0,    0],\n",
            "        [  36,    0,    0,    0,    0],\n",
            "        [   4,    0,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "lengths  tensor([10,  7,  7,  5,  4])\n",
            "target variable  tensor([[ 290,    4,   50,  147,   16],\n",
            "        [  27,    4,   47,   68, 3256],\n",
            "        [ 213,    4,    7,    7,    4],\n",
            "        [  12,  477,  260,  259,  147],\n",
            "        [3590,    4,   65,  174,   92],\n",
            "        [   6,    2,  331,    6,    7],\n",
            "        [   2,    0,  117,    2,    6],\n",
            "        [   0,    0,  401,    0,    2],\n",
            "        [   0,    0,    6,    0,    0],\n",
            "        [   0,    0,    2,    0,    0]])\n",
            "mask  tensor([[1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 0, 1, 1, 1],\n",
            "        [0, 0, 1, 0, 1],\n",
            "        [0, 0, 1, 0, 0],\n",
            "        [0, 0, 1, 0, 0]], dtype=torch.uint8)\n",
            "max target len 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ds9xvTpqOjgt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Models\n",
        "\n",
        "\n",
        "The main model is a seq-to-seq model, which takes a variable length sequence as an input and return a variable length sequence as output  using a fixed size model\n",
        "\n",
        "Two separate RNN's  running together . One acts as a **encoder** , encoding a variable length input sequence to a fixed length context vector . This context vector\n",
        "( final hidden layer of the RNN) will contain semantic info about the query sentence that is input.\n",
        "\n",
        "The second RNN acts as a **decoder** which takes an input word and context vector and returns a guess for the next word in the sequence and a hidden state to use in next iteration\n"
      ]
    },
    {
      "metadata": {
        "id": "MPmrna2TOyOR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Encoder **\n",
        "\n",
        "\n",
        "Encoder RNN iterates through the input sentence one token at a time and outputs a hidden state vector and output vector at each time step .\n",
        "\n",
        "Hidden state is passed to the next time step while ouput vector is recorded . Transforms the context into a set of points in high dimensional space , which decoder uses for  generating output\n",
        "\n",
        "**GRU**\n",
        "\n",
        "Using a bidirectional Gated Recurrent Unit (GRU) ; meaning essentially there are 2 independent RNN's . One fed the input sequence in normal sequential order and other in opposite order . The outputs of both are summed at each time step ensuring past and future context\n",
        "\n",
        "Using embedding layer to encode word indexes into arbitrary feature space. Here it will map each word to a feature space of size hidden_size \n",
        "\n",
        "* If passing a padded batch of sequence into an RNN module , we must pack and unpack padding around the RNN pass using **torch.nn.utils.rnn.pack_padded_sequence** and **torch.nn.utils.rnn.pad_packed_sequence** respectively\n",
        "\n",
        "Computation Graph \n",
        "\n",
        "\n",
        "1.   Convert word indexes to embeddings\n",
        "2.   Pack padded batch of sequence for RNN module\n",
        "3.   Forward pass through GRU\n",
        "4.   Unpack Padding\n",
        "5.   Sum bidirectional GRU outputs\n",
        "6.  Return output and final hidden state\n",
        "\n",
        "\n",
        "Inputs :\n",
        "* input_seq: batch of input sentences;shape=(max_length,batch_size)\n",
        "* input_lengths :list of sentence lengths corresponding to each sentence in the batch; shape=(batch_size)\n",
        "* hidden:hidden state shape=(n_layers x num_directions, batch_size, hidden_size)\n",
        "\n",
        "Outputs:\n",
        "* outputs : output features of the last hidden layer of the GRU ( sum of bidirectional outputs);  shape=(max_length,batch_size,hidden_size)\n",
        "* hidden : updated hidden state from GRU; shape = (n_layers x num_directionsm batch_size, hidden_size)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "05xN68xVSl11",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self,hidden_state,embedding,n_layers=1,dropout=0):\n",
        "    super(EncoderRNN,self).__init__()\n",
        "    self.n_layers = n_layers\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = embedding\n",
        "    \n",
        "    # initalize GRU - the input size and hidden size params are both set to hidden params\n",
        "    # as the input size is a word embedding with no of features == hidden_size\n",
        "    self.gru = nn.GRU(hidden_size,hidden_size,n_layers,\n",
        "                      dropout=(0 if n_layers==1 else dropout),bidirectional=True)\n",
        "    \n",
        "  def forward(self,input_seq,input_lengths,hidden=None):\n",
        "    # convert word indexes to embeddings\n",
        "    embedded = self.embedding(input_seq)\n",
        "    # pack padded batch of sequences for RNN module\n",
        "    packed =  torch.nn.utils.rnn.pack_padded_sequence(embedded,input_lengths)\n",
        "    # forward pass through GRU\n",
        "    outputs, hidden = self.gru(packed,hidden)\n",
        "    # unpack padding\n",
        "    outputs,_ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "    # sum bidirectional GRU outputs\n",
        "    outputs = outputs[:,:,:self.hidden_size]+outputs[:,:,self.hidden_size:]\n",
        "    # return output and final hidden state\n",
        "    return outputs , hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bc-gQwIXWvdF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Decoder **\n",
        "\n",
        "A decoder RNN generates sentence in  a tken-by-token fashion using encoder's context vectors and internal hidden states to generate the next word in the sequence. \n",
        "\n",
        "It generates tokens until it reaches an EOS_token.\n",
        "\n",
        "Problem with vanilla seq-to-seq decoder - information loss especially with long input sequences , limiting the capability of the decoder \n",
        "\n",
        "** Attention Mechanism** allows decoder to look at certain parts of input rather than the entire fixed context at every step.\n",
        "\n",
        "** Attention **  is calculated using decoder's current hidden state and the encoder's outputs.\n",
        "The output attention weights have same shape as the input sequence allowing them to be multiplied by the encoder's outputs giving a weighted sum which indicates the part of the encoder output to pay attention to.\n",
        "\n",
        "** Global Attention ** consider all of the encoder's hidden states ( instead of the hidden state of current time step) \n",
        "\n",
        "Also calculate attention  weights using the hidden state of the decoder from the current time step only ( local requires knowledge from previous time step)\n",
        "\n",
        "Score functions \n",
        "\n",
        "Implement attention layer as a separate nn.Module called Attn - o/p is a softmax normalized weights tensor of shape ( batch_size,1,max_length)"
      ]
    },
    {
      "metadata": {
        "id": "b4wNF8gSyV8w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Attn(nn.Module):\n",
        "  def __init__(self,method,hidden_size):\n",
        "    super(Attn,self).__init__()\n",
        "    self.method = method\n",
        "    if self.method not in ['dot','general','concat']:\n",
        "      raise ValueError(self.method,\" is not an appropriate attention method\")\n",
        "    self.hidden_size = hidden_size\n",
        "    if self.method == 'general':\n",
        "      self.attn = torch.nn.Linear(self.hidden_size,hidden_size)\n",
        "    elif self.method == 'concat':\n",
        "      self.attn = torch.nn.Linear(self.hidden_size*2,hidden_size)\n",
        "      self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "  \n",
        "  def dot_score(self,hidden,encoder_output):\n",
        "    return torch.sum(hidden * encoder_output,dim=2)\n",
        "  \n",
        "  def general_score(self,hidden,encoder_output):\n",
        "    energy = self.attn(encoder_output)\n",
        "    return torch.sum(hidden*energy,dim=2)\n",
        "  \n",
        "  def concat_score(self,hidden,encoder_output):\n",
        "    energy = self.attn(torch.cat(\n",
        "        (hidden.expand(encoder_output.size(0),-1,-1),encoder_output),2)).tanh()\n",
        "    return torch.sum(self.v*energy,dim=2)\n",
        "  \n",
        "  def forward(self,hidden,encoder_outputs):\n",
        "    # calculate the attention weights according to method\n",
        "    if self.method == 'general':\n",
        "      self.energies = self.general_score(hidden,encoder_outputs)\n",
        "    elif self.method == 'concat':\n",
        "      self.energies = self.concat_score(hidden,encoder_outputs)\n",
        "    elif self.method == 'dot':\n",
        "      attn_energies = self.dot_score(hidden,encoder_outputs)\n",
        "      \n",
        "      \n",
        "    # transpose max length and batch size dimensions \n",
        "    attn_energies = attn_energies.t()\n",
        "    \n",
        "    # return the softmax normalied probabiity scores (with added dimension)\n",
        "    return F.softmax(attn_energies,dim=1).unsqueeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rHgpWP4gEzMb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Implementing the actual decoder  model  . Manually feed our batch one time step at a time . Thus embedded word tensor and GRU output will both have shape(1,batch_size,hidden_size)\n",
        "\n",
        "Computation Graph :\n",
        "\n",
        "\n",
        "1.   Get embedding of current input word\n",
        "2.   Forward through unidirectional GRU\n",
        "3.   Calculate attention weights from current GRU output from (2)\n",
        "4.   Multiply attention weights to encoder outputs to get new weighted sum context vector\n",
        "5.   Concatenate weighted context vector and GRU output \n",
        "6.   Predict next word using Luong eg ( without softmax)\n",
        "7.   Return output and final hidden state\n",
        "\n",
        "\n",
        "Inputs:\n",
        "\n",
        "* input_step :one tie step  (one word) of input sequence batch ; shape(1,batch_size)\n",
        "* last_hidden : final hidden layer of GRU ; shape (n_layers x num_directions , batch_size,hidden_size )\n",
        "* encoder_outputs : encoder model's output; shape ( max_length,batch_size,hidden_size)\n",
        "\n",
        "Outputs:\n",
        "\n",
        "*  output : softmax normalised tensor giving probabilities to each word being the correct next word in the decoded sequence ; shape(batc_size,voc.num_words)\n",
        "* hidden : final hidden state of GRU; shape (n_layers x num_directions ,batch_size , hidden_size)\n"
      ]
    },
    {
      "metadata": {
        "id": "Jry59xGbkS9g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "  def __init__(self,attn_model,embedding,hidden_size,output_size,n_layers=1,dropout=0.1):\n",
        "    super(LuongAttnDecoderRNN,self).__init__()\n",
        "    # keep for reference \n",
        "    self.attn_model = attn_model\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "    self.dropout = dropout\n",
        "    \n",
        "    # define layers \n",
        "    self.embedding = embedding\n",
        "    self.embedding_dropout = nn.Dropout(dropout)\n",
        "    self.gru = nn.GRU(hidden_size,hidden_size,n_layers,dropout=(0 if n_layers==1 else dropout))\n",
        "    self.concat = nn.Linear(hidden_size*2,hidden_size)\n",
        "    self.out = nn.Linear(hidden_size,output_size)\n",
        "    self.attn = Attn(attn_model,hidden_size)\n",
        " \n",
        "  def forward(self,input_step,last_hidden,encoder_outputs):\n",
        "    # Run thhs one step(word) at a time \n",
        "    # Get embedding of current input word\n",
        "    embedded = self.embedding(input_step)\n",
        "    emebdded = self.embedding_dropout(embedded)\n",
        "    # Forward through unidirectional GRU\n",
        "    rnn_output,hidden = self.gru(embedded,last_hidden)\n",
        "    # calculate attention weights from current GRU output\n",
        "    attn_weights = self.attn(rnn_output,encoder_outputs)\n",
        "    # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "    context = attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
        "    # Concatenat eweighted context vector and GRU output using Luong eq\n",
        "    rnn_output = rnn_output.squeeze(0)\n",
        "    context = context.squeeze(1)\n",
        "    concat_input = torch.cat((rnn_output,context),1)\n",
        "    concat_output = torch.tanh(self.concat(concat_input))\n",
        "    # Predict next word using Luong\n",
        "    output = self.out(concat_output)\n",
        "    output = F.softmax(output,dim=1)\n",
        "    # Return output and final hidden state\n",
        "    return output,hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y1M4NmBRoXy6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining Training Procedure\n",
        "\n",
        "Masked Loss : Since dealing with batches of padded sequences , cannot consider all elements of tensor when calculating loss . Define \"maskNLLLoss\" to calculate loss based on decoder's output tensor , the target tensor and a binary mask tensor describing the padding of the target tensor. This loss function calculates the average negetive log liklihood of elements that correspond to a 1 in the mask tensor"
      ]
    },
    {
      "metadata": {
        "id": "Bn-7qlFhpKRa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def maskNLLLoss(inp,target,mask):\n",
        "  nTotal = mask.sum()\n",
        "  crossEntropy = -torch.log(torch.gather(inp,1,target.view(-1,1)))\n",
        "  loss = crossEntropy.masked_select(mask).mean()\n",
        "  loss = loss.to(device)\n",
        "  return loss,nTotal.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dThiHH-Or24W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Single Iteration  Training**\n",
        "\n",
        "train - algo for single training iteration ( single batch)\n",
        "\n",
        "Tricks to aid in convergence:\n",
        " \n",
        " * **Teacher Forcing** - at some probability , set by **teacher_forcing_ratio**, use current target word as decoder's next input rather than using decoder's current guess . Aids in more efficient training , can cause problems during inference \n",
        " \n",
        " * **Gradient Clipping ** - Counter the exploading gradient problem.\n",
        " \n",
        " \n",
        " Sequence Of Operations :\n",
        " \n",
        "1. Forward pass entire input batch through encoder   \n",
        "2. Initialize decoder inputs as SOS_token and hidden state as the encoder's final hidden state\n",
        "3. Forward input batch sequence through decoder one time step at a time \n",
        "4. If teacher forcing: set next ecoder input as the current targer ; else:set next excoder input as the current decoder output\n",
        "5. Calculate and accumulate loss\n",
        "6. Perform backprop\n",
        "7. Clip Gradients\n",
        "8. Update encoder and decoder model parameters\n",
        "\n",
        "\n",
        "* Pytorch's RNN models can be used by passing entire input seq or one time step at a time\n"
      ]
    },
    {
      "metadata": {
        "id": "9vr5GKqnuFkm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(input_variable,lengths,target_variable,mask,max_target_len,encoder,decoder,\n",
        "          embedding,encoder_optimizer,decoder_optimizer,batch_size,clip,max_length=MAX_LENGTH):\n",
        "  # Zero Gradients \n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "  \n",
        "  # set device options \n",
        "  input_variable = input_variable.to(device)\n",
        "  lengths = lengths.to(device)\n",
        "  target_variable = target_variable.to(device)\n",
        "  mask = mask.to(device)\n",
        "  \n",
        "  # initialize variables\n",
        "  loss = 0\n",
        "  print_losses =[]\n",
        "  n_totals=0\n",
        "  \n",
        "  # Forward Pass through encoder\n",
        "  encoder_outputs , encoder_hidden = encoder(input_variable,lengths)\n",
        "  \n",
        "  # Create initial deccoder input (start with SOS_token for each sentence)\n",
        "  decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "  decoder_input = decoder_input.to(device)\n",
        "  \n",
        "  # set initial decoder hidden state to the encoder's final hidden state\n",
        "  decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "  \n",
        "  # Determine if teacher forcing is used in this iteration\n",
        "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "  \n",
        "  # Forward batch of sequences through decoder one time step at a time\n",
        "  if use_teacher_forcing:\n",
        "    for t in range(max_target_len):\n",
        "      decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
        "      # Teacher forcing is on thus next input is current target\n",
        "      decoder_input = target_variable[t].view(1,-1)\n",
        "      # calculate and accumulate loss\n",
        "      mask_loss,nTotal = maskNLLLoss(decoder_output,target_variable[t],mask[t])\n",
        "      loss +=mask_loss\n",
        "      print_losses.append(mask_loss.item()*nTotal)\n",
        "      n_totals += nTotal\n",
        "  else:\n",
        "    for t in range(max_target_len):\n",
        "      decoder_output,decoder_hidden = decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
        "      # No teacher forcing : next input is decoder's own current output\n",
        "      _, topi = decoder_output.topk(1)\n",
        "      decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "      decoder_input = decoder_input.to(device)\n",
        "      # Calculate and accumulate loss\n",
        "      mask_loss,nTotal = maskNLLLoss(decoder_output,target_variable[t],mask[t])\n",
        "      loss +=mask_loss\n",
        "      print_losses.append(mask_loss.item()*nTotal)\n",
        "      n_totals += nTotal\n",
        "  \n",
        "  # Perform backprop\n",
        "  loss.backward()\n",
        "  \n",
        "  # Clip gradients: _ modified in place \n",
        "  _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(),clip)\n",
        "  _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(),clip)\n",
        "  \n",
        "  # Adjust model weights\n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "  \n",
        "  return sum(print_losses)/n_totals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0wh7aFPW2O8R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Training Iterations **\n",
        "\n",
        "trainingIters -  running n_iterations given models , data , optimizers  \n",
        "\n",
        "Saving model weights - encoder and decoder state_dict (parameters ) , optimizer's state_dicts , the loss , the iteration etc"
      ]
    },
    {
      "metadata": {
        "id": "sIontThJ3Bil",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainIters(model_name,voc,pairs,encoder,decoder,encoder_optimizer,decoder_optimizer,\n",
        "               embedding,encoder_n_layers,decoder_n_layers,save_dir,n_iteration,batch_size,\n",
        "              print_every,save_every,clip,corpus_name,loadFilename):\n",
        "  \n",
        "  # Load batches for each iteration\n",
        "  training_batches = [batch2TrainData(voc,[random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)]\n",
        "  \n",
        "  # Initilizing \n",
        "  print(\"initializing\")\n",
        "  start_iteration =1 \n",
        "  print_loss =0\n",
        "  if loadFilename:\n",
        "    start_iteration = checkpoint['iteration']+1\n",
        " \n",
        "  # Training Loop\n",
        "  print(\"training\")\n",
        "  for iteration in range(start_iteration,n_iteration +1):\n",
        "    training_batch = training_batches[iteration -1]\n",
        "    # Extract fields from batch\n",
        "    input_variable , lengths, target_variable, mask , max_target_len = training_batch\n",
        "    \n",
        "    # Run a training iteration with batch\n",
        "    loss =  train(input_variable,lengths,target_variable,mask,max_target_len,encoder,\n",
        "                  decoder,embedding,encoder_optimizer,decoder_optimizer,batch_size,clip)\n",
        "    print_loss +=loss\n",
        "    \n",
        "    # Print Progress\n",
        "    if iteration % print_every == 0:\n",
        "      print_loss_avg = print_loss/print_every\n",
        "      print(f\"Iteration {iteration}; Percent Complete {iteration/n_iteration *100} ; Avg Loss {print_loss_avg}\")\n",
        "      print_loss =0\n",
        "    \n",
        "    # save checkpoint\n",
        "    if iteration %save_every == 0:\n",
        "      print('skipping saving for now ')\n",
        "#       directory = os.path.join(save_dir,model_name,corpus_name,f\"{encoder_n_layers}_{decoder_n_layers}_{hidden_size}\")\n",
        "#       if not os.path.exists(directory):\n",
        "#         os.mkdir(directory)\n",
        "#       torch.save({\n",
        "#           'iteration':iteration,\n",
        "#           'en':encoder.state_dict(),\n",
        "#           'de':decoder.state_dict(),\n",
        "#           'en_opt':encoder_optimizer.state_dict(),\n",
        "#           'de_opt':decoder_optimizer.state_dict(),\n",
        "#           'loss':loss,\n",
        "#           'voc_dict':voc.__dict__,\n",
        "#           'embedding':embedding.state_dict()\n",
        "#       },os.path.join(dirctory,f'{iteration}_{checkpint}.tar'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r-ZF9fSmOts9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Define Evaluation **\n",
        "\n",
        "After training need to decode the encoded input \n",
        "\n",
        "**Greedy decoding **\n",
        "\n",
        "method used during training when not using teacher forcing . For each time step simply choose the word from decoder_output wih highest softmax value ( Optimal on single time-step-level)\n",
        "\n",
        "GreedySearchDecoder - takes input_seq ( shape - (input_seq_length ,1) ) , a scaler input_length tensor and a max_length to bound the response sentence length\n",
        "\n",
        "Computation Graph\n",
        "\n",
        "\n",
        "1.   Forward input through encoder model\n",
        "2.   Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "3.   Initialize decoder's first input as SOS_token\n",
        "4.   Initialize tensors to append decoded words to\n",
        "5. **Iteratively decode one word token at a time **\n",
        "        \n",
        "\n",
        "        1. Forward pass through decoder\n",
        "        2. Obtain most likely word token and its softmax score\n",
        "        3. Record token and score\n",
        "        4. Prepare current token to be next decoder input\n",
        "\n",
        "6.   Return collections of word tokens and scores.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "pkRDBoj7Q0mR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "  def __init__(self,encoder,decoder):\n",
        "    super(GreedySearchDecoder,self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    \n",
        "  def forward(self,input_seq,input_length,max_length):\n",
        "    # Forward input through encoder model \n",
        "    encoder_outputs , encoder_hidden = self.encoder(input_seq,input_length)\n",
        "    # Prepare encoder's final hidden layer to be the first hidden input to the decoder\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "    # Initialize decoer input with SOS_token\n",
        "    decoder_input = torch.ones(1,1,device=device,dtype=torch.long) * SOS_token\n",
        "    # initialize tensors to append decoded words to \n",
        "    all_tokens = torch.zeros([0],device=device,dtype=torch.long)\n",
        "    all_scores = torch.zeros([0],device=device)\n",
        "    \n",
        "    # Iteratively decode one word token at a time \n",
        "    for _ in range(max_length):\n",
        "      # forward pass through decoder\n",
        "      decoder_output,decoder_hidden = self.decoder(decoder_input,decoder_hidden,encoder_outputs)\n",
        "      # Obtain most likelly word token and its softmax score\n",
        "      decoder_scores, decoder_input = torch.max(decoder_output,dim=1)\n",
        "      # Record token and score\n",
        "      all_tokens = torch.cat((all_tokens,decoder_input),dim=0)\n",
        "      all_scores = torch.cat((all_scores,decoder_scores),dim=0)\n",
        "      #Prepare current token to be next decoder input (add a dimension)\n",
        "      decoder_input = torch.unsqueeze(decoder_input,0)\n",
        "    # return collections of word tokens and scores\n",
        "    return all_tokens,all_scores\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GqyVzlWuTOn4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Evaluate My Text **\n",
        "\n",
        "\n",
        "Functions for evaluating a string input sentence .\n",
        "\n",
        "evaluate - manage the low level processing of handling the input sentence . Format the sentence as an input batch of word indexes with batch_size =1 by converting words of a sentence to their corresponding indexes and transposing the dimensions \n",
        "\n",
        "Also create a lengths tensor which contains the length of the input sentence , this case lengths a scalar as only evaluating one sentence at a time ( batch_size==1) \n",
        "\n",
        "Next obtain the decoded response sentence tensor using GreedySearchDecoder object . finally convert the response indexes to words and return decoded words\n",
        "\n",
        "evaluateInput - UI for the chatbot , enter query sentence . After text is normalized in the same way as training data it is fed to evaluate function to obtain a decoded output. Looping this function till q or quit is entered\n",
        "\n",
        "If sentence contains word not in vocaulary then print error message"
      ]
    },
    {
      "metadata": {
        "id": "2JLSYR3eXaty",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate(encoder,decoder,searcher,voc,sentence,max_length=MAX_LENGTH):\n",
        "  # Format input sentence as a batch \n",
        "  # words -> indexes\n",
        "  indexes_batch = [indexesFromSentence(voc,sentence)]\n",
        "  # Create lengths tensor\n",
        "  lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "  # Transpose dimensions of batch to match model's expectations \n",
        "  input_batch = torch.LongTensor(indexes_batch).transpose(0,1)\n",
        "  # Use appropriate device\n",
        "  input_batch= input_batch.to(device)\n",
        "  lengths = lengths.to(device)\n",
        "  # decode sentences with searcher\n",
        "  tokens,scores = searcher(input_batch,lengths,max_length)\n",
        "  # indexes -> words\n",
        "  decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "  return decoded_words\n",
        "\n",
        "def evaluateInput(encoder,decoder,searcher,voc):\n",
        "  input_sentence = ''\n",
        "  while(1):\n",
        "    try:\n",
        "      # Get input sentence\n",
        "      input_sentence = input('> ')\n",
        "      # check if quit case\n",
        "      if input_sentence == 'q' or input_sentence =='quit':break\n",
        "      #Normalize sentence\n",
        "      input_sentence = normalizeString(input_sentence)\n",
        "      # evaluate sentence\n",
        "      output_words = evaluate(encoder,decoder,searcher,voc,input_sentence)\n",
        "      # Format and print response\n",
        "      output_words[:] = [x for x in output_words if not(x=='EOS' or x =='PAD')]\n",
        "      print('BOT :',''.join(output_words))\n",
        "    except Exception as ex:\n",
        "      print(\"Error: encounterd unknown word\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j4sKHR2iaHV0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Run Model**\n",
        "\n",
        "Setting configurations "
      ]
    },
    {
      "metadata": {
        "id": "ixQQaUVmaPHT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5a6a114c-67cc-4b63-91c7-d4dc21d5e70f"
      },
      "cell_type": "code",
      "source": [
        "# configure models \n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "\n",
        "hidden_size = 500\n",
        "encoder_n_layers =3\n",
        "decoder_n_layers =3\n",
        "dropout=0.1\n",
        "batch_size = 64\n",
        "\n",
        "#checkpoint to oad from \n",
        "loadFilename =None\n",
        "checkpoint_iter =4000\n",
        "\n",
        "# Load model if model loadFileName is provided\n",
        "if loadFilename:\n",
        "  checkpoint = torch.load(loadFilename)\n",
        "  encoder_sd = checkpoint['en']\n",
        "  decoder_sd = checkpoint['de']\n",
        "  encoder_optimizer_sd = checkpoint['en_opt']\n",
        "  decoder_optimizer_sd = checkpoint['de_opt']\n",
        "  embedding_sd = checkpoint['embedding']\n",
        "  voc.__dict__ = checkpoint['voc_dict']\n",
        "  \n",
        "print('Building encoder and decoder')\n",
        "\n",
        "# initailize word embedding\n",
        "embedding = nn.Embedding(voc.num_words,hidden_size)\n",
        "if loadFilename:\n",
        "  embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder and decoder models\n",
        "encoder = EncoderRNN(hidden_size,embedding,encoder_n_layers,dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model,embedding,hidden_size,voc.num_words,decoder_n_layers,dropout)\n",
        "\n",
        "if loadFilename:\n",
        "  encoder.load_state_dict(encoder_sd)\n",
        "  decoder.load_state_dict(decoder_sd)\n",
        "\n",
        "# setting device \n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "print('Models built ')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building encoder and decoder\n",
            "Models built \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Et38sne6cLSw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Training the model**"
      ]
    },
    {
      "metadata": {
        "id": "cv7MiNJAcc33",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7021
        },
        "outputId": "75a55d57-7ad5-464a-b693-3208ad00975a"
      },
      "cell_type": "code",
      "source": [
        "# configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 10\n",
        "save_every = 500\n",
        "\n",
        "# ensure dropout layers are tain mode \n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initiaize optimizers \n",
        "print('Building optimizers')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(),lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(),lr=learning_rate)\n",
        "\n",
        "if loadFilename:\n",
        "  encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "  decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "print('Starting training')\n",
        "trainIters(model_name,voc,pairs,encoder,decoder,encoder_optimizer,decoder_optimizer,\n",
        "               embedding,encoder_n_layers,decoder_n_layers,save_dir,n_iteration,batch_size,\n",
        "              print_every,save_every,clip,corpus_name,loadFilename)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building optimizers\n",
            "Starting training\n",
            "initializing\n",
            "training\n",
            "Iteration 10; Percent Complete 0.25 ; Avg Loss 3.7026574423813905\n",
            "Iteration 20; Percent Complete 0.5 ; Avg Loss 3.661538867176567\n",
            "Iteration 30; Percent Complete 0.75 ; Avg Loss 3.606323970893451\n",
            "Iteration 40; Percent Complete 1.0 ; Avg Loss 3.691636301809086\n",
            "Iteration 50; Percent Complete 1.25 ; Avg Loss 3.6285671456545714\n",
            "Iteration 60; Percent Complete 1.5 ; Avg Loss 3.5709345787061793\n",
            "Iteration 70; Percent Complete 1.7500000000000002 ; Avg Loss 3.601548357626309\n",
            "Iteration 80; Percent Complete 2.0 ; Avg Loss 3.6081869173212686\n",
            "Iteration 90; Percent Complete 2.25 ; Avg Loss 3.5915704634440644\n",
            "Iteration 100; Percent Complete 2.5 ; Avg Loss 3.605436584652491\n",
            "Iteration 110; Percent Complete 2.75 ; Avg Loss 3.663495206419826\n",
            "Iteration 120; Percent Complete 3.0 ; Avg Loss 3.5958588502503916\n",
            "Iteration 130; Percent Complete 3.25 ; Avg Loss 3.574765725814595\n",
            "Iteration 140; Percent Complete 3.5000000000000004 ; Avg Loss 3.6564385856011077\n",
            "Iteration 150; Percent Complete 3.75 ; Avg Loss 3.641282735847086\n",
            "Iteration 160; Percent Complete 4.0 ; Avg Loss 3.580649534904442\n",
            "Iteration 170; Percent Complete 4.25 ; Avg Loss 3.549368018688525\n",
            "Iteration 180; Percent Complete 4.5 ; Avg Loss 3.551401612040363\n",
            "Iteration 190; Percent Complete 4.75 ; Avg Loss 3.5507412348137506\n",
            "Iteration 200; Percent Complete 5.0 ; Avg Loss 3.5619913485434522\n",
            "Iteration 210; Percent Complete 5.25 ; Avg Loss 3.4597595201367524\n",
            "Iteration 220; Percent Complete 5.5 ; Avg Loss 3.4723596526656038\n",
            "Iteration 230; Percent Complete 5.75 ; Avg Loss 3.6134089403792373\n",
            "Iteration 240; Percent Complete 6.0 ; Avg Loss 3.5138273718780426\n",
            "Iteration 250; Percent Complete 6.25 ; Avg Loss 3.512855162683009\n",
            "Iteration 260; Percent Complete 6.5 ; Avg Loss 3.547600222166536\n",
            "Iteration 270; Percent Complete 6.75 ; Avg Loss 3.4575014294393114\n",
            "Iteration 280; Percent Complete 7.000000000000001 ; Avg Loss 3.5110793346280267\n",
            "Iteration 290; Percent Complete 7.249999999999999 ; Avg Loss 3.475195983387472\n",
            "Iteration 300; Percent Complete 7.5 ; Avg Loss 3.538277337497558\n",
            "Iteration 310; Percent Complete 7.75 ; Avg Loss 3.5185202488796605\n",
            "Iteration 320; Percent Complete 8.0 ; Avg Loss 3.433435147790631\n",
            "Iteration 330; Percent Complete 8.25 ; Avg Loss 3.5174423160534047\n",
            "Iteration 340; Percent Complete 8.5 ; Avg Loss 3.455367636631761\n",
            "Iteration 350; Percent Complete 8.75 ; Avg Loss 3.4153125986560937\n",
            "Iteration 360; Percent Complete 9.0 ; Avg Loss 3.4441241023925864\n",
            "Iteration 370; Percent Complete 9.25 ; Avg Loss 3.3914433271598923\n",
            "Iteration 380; Percent Complete 9.5 ; Avg Loss 3.440841271331796\n",
            "Iteration 390; Percent Complete 9.75 ; Avg Loss 3.4839957033124316\n",
            "Iteration 400; Percent Complete 10.0 ; Avg Loss 3.4123228579829936\n",
            "Iteration 410; Percent Complete 10.25 ; Avg Loss 3.470044334574351\n",
            "Iteration 420; Percent Complete 10.5 ; Avg Loss 3.399773864680051\n",
            "Iteration 430; Percent Complete 10.75 ; Avg Loss 3.413812646454365\n",
            "Iteration 440; Percent Complete 11.0 ; Avg Loss 3.377085915384046\n",
            "Iteration 450; Percent Complete 11.25 ; Avg Loss 3.3246253790773963\n",
            "Iteration 460; Percent Complete 11.5 ; Avg Loss 3.4323417094995228\n",
            "Iteration 470; Percent Complete 11.75 ; Avg Loss 3.3942048586889486\n",
            "Iteration 480; Percent Complete 12.0 ; Avg Loss 3.3839184292426863\n",
            "Iteration 490; Percent Complete 12.25 ; Avg Loss 3.3674654454968804\n",
            "Iteration 500; Percent Complete 12.5 ; Avg Loss 3.3673458429211025\n",
            "skipping saving for now \n",
            "Iteration 510; Percent Complete 12.75 ; Avg Loss 3.4443698821208555\n",
            "Iteration 520; Percent Complete 13.0 ; Avg Loss 3.400561721711965\n",
            "Iteration 530; Percent Complete 13.25 ; Avg Loss 3.3753959306864942\n",
            "Iteration 540; Percent Complete 13.5 ; Avg Loss 3.3584166336050223\n",
            "Iteration 550; Percent Complete 13.750000000000002 ; Avg Loss 3.3005642094223804\n",
            "Iteration 560; Percent Complete 14.000000000000002 ; Avg Loss 3.434517586508271\n",
            "Iteration 570; Percent Complete 14.249999999999998 ; Avg Loss 3.331377544362155\n",
            "Iteration 580; Percent Complete 14.499999999999998 ; Avg Loss 3.3241047083162414\n",
            "Iteration 590; Percent Complete 14.75 ; Avg Loss 3.3454031837942573\n",
            "Iteration 600; Percent Complete 15.0 ; Avg Loss 3.4505565924784163\n",
            "Iteration 610; Percent Complete 15.25 ; Avg Loss 3.4236184943599364\n",
            "Iteration 620; Percent Complete 15.5 ; Avg Loss 3.3496896608543842\n",
            "Iteration 630; Percent Complete 15.75 ; Avg Loss 3.336325536340778\n",
            "Iteration 640; Percent Complete 16.0 ; Avg Loss 3.3807175484717775\n",
            "Iteration 650; Percent Complete 16.25 ; Avg Loss 3.3525917319340595\n",
            "Iteration 660; Percent Complete 16.5 ; Avg Loss 3.364605596623716\n",
            "Iteration 670; Percent Complete 16.75 ; Avg Loss 3.3802510544430553\n",
            "Iteration 680; Percent Complete 17.0 ; Avg Loss 3.28185070155858\n",
            "Iteration 690; Percent Complete 17.25 ; Avg Loss 3.330984953154563\n",
            "Iteration 700; Percent Complete 17.5 ; Avg Loss 3.2936778360420496\n",
            "Iteration 710; Percent Complete 17.75 ; Avg Loss 3.3048564865664942\n",
            "Iteration 720; Percent Complete 18.0 ; Avg Loss 3.3277394583532383\n",
            "Iteration 730; Percent Complete 18.25 ; Avg Loss 3.3099498156118523\n",
            "Iteration 740; Percent Complete 18.5 ; Avg Loss 3.390964881431511\n",
            "Iteration 750; Percent Complete 18.75 ; Avg Loss 3.3192787716671077\n",
            "Iteration 760; Percent Complete 19.0 ; Avg Loss 3.2463676550177722\n",
            "Iteration 770; Percent Complete 19.25 ; Avg Loss 3.2920379776139574\n",
            "Iteration 780; Percent Complete 19.5 ; Avg Loss 3.2831053488280224\n",
            "Iteration 790; Percent Complete 19.75 ; Avg Loss 3.245271251710278\n",
            "Iteration 800; Percent Complete 20.0 ; Avg Loss 3.166829089451654\n",
            "Iteration 810; Percent Complete 20.25 ; Avg Loss 3.194586935265346\n",
            "Iteration 820; Percent Complete 20.5 ; Avg Loss 3.271121954343394\n",
            "Iteration 830; Percent Complete 20.75 ; Avg Loss 3.287539847355312\n",
            "Iteration 840; Percent Complete 21.0 ; Avg Loss 3.285093772862145\n",
            "Iteration 850; Percent Complete 21.25 ; Avg Loss 3.263604235802597\n",
            "Iteration 860; Percent Complete 21.5 ; Avg Loss 3.341097501965379\n",
            "Iteration 870; Percent Complete 21.75 ; Avg Loss 3.2569841690576404\n",
            "Iteration 880; Percent Complete 22.0 ; Avg Loss 3.2505015295635418\n",
            "Iteration 890; Percent Complete 22.25 ; Avg Loss 3.264766323995585\n",
            "Iteration 900; Percent Complete 22.5 ; Avg Loss 3.3296309324005895\n",
            "Iteration 910; Percent Complete 22.75 ; Avg Loss 3.299119658108102\n",
            "Iteration 920; Percent Complete 23.0 ; Avg Loss 3.263850680112257\n",
            "Iteration 930; Percent Complete 23.25 ; Avg Loss 3.218403717850444\n",
            "Iteration 940; Percent Complete 23.5 ; Avg Loss 3.3066159363627583\n",
            "Iteration 950; Percent Complete 23.75 ; Avg Loss 3.2273906728474473\n",
            "Iteration 960; Percent Complete 24.0 ; Avg Loss 3.180835923591515\n",
            "Iteration 970; Percent Complete 24.25 ; Avg Loss 3.259443055947576\n",
            "Iteration 980; Percent Complete 24.5 ; Avg Loss 3.160343530365977\n",
            "Iteration 990; Percent Complete 24.75 ; Avg Loss 3.247567723769179\n",
            "Iteration 1000; Percent Complete 25.0 ; Avg Loss 3.2511463357809576\n",
            "skipping saving for now \n",
            "Iteration 1010; Percent Complete 25.25 ; Avg Loss 3.259802723060516\n",
            "Iteration 1020; Percent Complete 25.5 ; Avg Loss 3.227774812817164\n",
            "Iteration 1030; Percent Complete 25.75 ; Avg Loss 3.192618463978956\n",
            "Iteration 1040; Percent Complete 26.0 ; Avg Loss 3.201588532725502\n",
            "Iteration 1050; Percent Complete 26.25 ; Avg Loss 3.325018161052957\n",
            "Iteration 1060; Percent Complete 26.5 ; Avg Loss 3.244957164194063\n",
            "Iteration 1070; Percent Complete 26.75 ; Avg Loss 3.127925021884048\n",
            "Iteration 1080; Percent Complete 27.0 ; Avg Loss 3.2409713906259645\n",
            "Iteration 1090; Percent Complete 27.250000000000004 ; Avg Loss 3.1397636766045123\n",
            "Iteration 1100; Percent Complete 27.500000000000004 ; Avg Loss 3.275076376004386\n",
            "Iteration 1110; Percent Complete 27.750000000000004 ; Avg Loss 3.1949204570866123\n",
            "Iteration 1120; Percent Complete 28.000000000000004 ; Avg Loss 3.136623840382169\n",
            "Iteration 1130; Percent Complete 28.249999999999996 ; Avg Loss 3.1440960586207027\n",
            "Iteration 1140; Percent Complete 28.499999999999996 ; Avg Loss 3.2312299925178527\n",
            "Iteration 1150; Percent Complete 28.749999999999996 ; Avg Loss 3.151204844853031\n",
            "Iteration 1160; Percent Complete 28.999999999999996 ; Avg Loss 3.1463710092712844\n",
            "Iteration 1170; Percent Complete 29.25 ; Avg Loss 3.228558980196167\n",
            "Iteration 1180; Percent Complete 29.5 ; Avg Loss 3.2728786594563757\n",
            "Iteration 1190; Percent Complete 29.75 ; Avg Loss 3.247058210146784\n",
            "Iteration 1200; Percent Complete 30.0 ; Avg Loss 3.234237194461836\n",
            "Iteration 1210; Percent Complete 30.25 ; Avg Loss 3.2788947272576037\n",
            "Iteration 1220; Percent Complete 30.5 ; Avg Loss 3.089835886159709\n",
            "Iteration 1230; Percent Complete 30.75 ; Avg Loss 3.1351191739250477\n",
            "Iteration 1240; Percent Complete 31.0 ; Avg Loss 3.142593060788004\n",
            "Iteration 1250; Percent Complete 31.25 ; Avg Loss 3.2617148518534576\n",
            "Iteration 1260; Percent Complete 31.5 ; Avg Loss 3.171159728565696\n",
            "Iteration 1270; Percent Complete 31.75 ; Avg Loss 3.175819505415503\n",
            "Iteration 1280; Percent Complete 32.0 ; Avg Loss 3.2451035304803413\n",
            "Iteration 1290; Percent Complete 32.25 ; Avg Loss 3.1270244910392115\n",
            "Iteration 1300; Percent Complete 32.5 ; Avg Loss 3.1690730420579944\n",
            "Iteration 1310; Percent Complete 32.75 ; Avg Loss 3.149708200828988\n",
            "Iteration 1320; Percent Complete 33.0 ; Avg Loss 3.1883003266429473\n",
            "Iteration 1330; Percent Complete 33.25 ; Avg Loss 3.141571951770543\n",
            "Iteration 1340; Percent Complete 33.5 ; Avg Loss 3.1829305350444366\n",
            "Iteration 1350; Percent Complete 33.75 ; Avg Loss 3.2609440247650205\n",
            "Iteration 1360; Percent Complete 34.0 ; Avg Loss 3.2100391490871814\n",
            "Iteration 1370; Percent Complete 34.25 ; Avg Loss 3.210659392468643\n",
            "Iteration 1380; Percent Complete 34.5 ; Avg Loss 3.143005602252205\n",
            "Iteration 1390; Percent Complete 34.75 ; Avg Loss 3.2023052260121694\n",
            "Iteration 1400; Percent Complete 35.0 ; Avg Loss 3.1913727725319374\n",
            "Iteration 1410; Percent Complete 35.25 ; Avg Loss 3.152187778932677\n",
            "Iteration 1420; Percent Complete 35.5 ; Avg Loss 3.2469055972607395\n",
            "Iteration 1430; Percent Complete 35.75 ; Avg Loss 3.1961300153331216\n",
            "Iteration 1440; Percent Complete 36.0 ; Avg Loss 3.182848595895016\n",
            "Iteration 1450; Percent Complete 36.25 ; Avg Loss 3.2330703867345725\n",
            "Iteration 1460; Percent Complete 36.5 ; Avg Loss 3.208087074390265\n",
            "Iteration 1470; Percent Complete 36.75 ; Avg Loss 3.1280243533842906\n",
            "Iteration 1480; Percent Complete 37.0 ; Avg Loss 3.1096590148561325\n",
            "Iteration 1490; Percent Complete 37.25 ; Avg Loss 3.0789178290109804\n",
            "Iteration 1500; Percent Complete 37.5 ; Avg Loss 3.103766184809065\n",
            "skipping saving for now \n",
            "Iteration 1510; Percent Complete 37.75 ; Avg Loss 3.1382781068108363\n",
            "Iteration 1520; Percent Complete 38.0 ; Avg Loss 3.1823775200903373\n",
            "Iteration 1530; Percent Complete 38.25 ; Avg Loss 3.1444408516011126\n",
            "Iteration 1540; Percent Complete 38.5 ; Avg Loss 3.1796091498742944\n",
            "Iteration 1550; Percent Complete 38.75 ; Avg Loss 3.1672396149694215\n",
            "Iteration 1560; Percent Complete 39.0 ; Avg Loss 3.1345145417487914\n",
            "Iteration 1570; Percent Complete 39.25 ; Avg Loss 3.126221384453143\n",
            "Iteration 1580; Percent Complete 39.5 ; Avg Loss 3.196861563257795\n",
            "Iteration 1590; Percent Complete 39.75 ; Avg Loss 3.0932689747199684\n",
            "Iteration 1600; Percent Complete 40.0 ; Avg Loss 3.0794168504009174\n",
            "Iteration 1610; Percent Complete 40.25 ; Avg Loss 3.091344599452117\n",
            "Iteration 1620; Percent Complete 40.5 ; Avg Loss 3.0780538342215893\n",
            "Iteration 1630; Percent Complete 40.75 ; Avg Loss 3.095350286471194\n",
            "Iteration 1640; Percent Complete 41.0 ; Avg Loss 3.1528609242598145\n",
            "Iteration 1650; Percent Complete 41.25 ; Avg Loss 3.1412326301158284\n",
            "Iteration 1660; Percent Complete 41.5 ; Avg Loss 3.1375451237591703\n",
            "Iteration 1670; Percent Complete 41.75 ; Avg Loss 3.027585126996254\n",
            "Iteration 1680; Percent Complete 42.0 ; Avg Loss 3.0743512271997493\n",
            "Iteration 1690; Percent Complete 42.25 ; Avg Loss 3.067124518437023\n",
            "Iteration 1700; Percent Complete 42.5 ; Avg Loss 3.188057462505806\n",
            "Iteration 1710; Percent Complete 42.75 ; Avg Loss 3.0665131893806445\n",
            "Iteration 1720; Percent Complete 43.0 ; Avg Loss 3.1581348791282293\n",
            "Iteration 1730; Percent Complete 43.25 ; Avg Loss 3.1343498550638236\n",
            "Iteration 1740; Percent Complete 43.5 ; Avg Loss 3.089341173093576\n",
            "Iteration 1750; Percent Complete 43.75 ; Avg Loss 3.053580120382675\n",
            "Iteration 1760; Percent Complete 44.0 ; Avg Loss 3.0317257257237467\n",
            "Iteration 1770; Percent Complete 44.25 ; Avg Loss 3.1353546572389956\n",
            "Iteration 1780; Percent Complete 44.5 ; Avg Loss 3.10873008940224\n",
            "Iteration 1790; Percent Complete 44.75 ; Avg Loss 2.999184341776946\n",
            "Iteration 1800; Percent Complete 45.0 ; Avg Loss 3.0429185660078693\n",
            "Iteration 1810; Percent Complete 45.25 ; Avg Loss 3.0412693958760917\n",
            "Iteration 1820; Percent Complete 45.5 ; Avg Loss 3.215384725038011\n",
            "Iteration 1830; Percent Complete 45.75 ; Avg Loss 3.050234429241723\n",
            "Iteration 1840; Percent Complete 46.0 ; Avg Loss 3.1202458122798133\n",
            "Iteration 1850; Percent Complete 46.25 ; Avg Loss 3.0255130475534306\n",
            "Iteration 1860; Percent Complete 46.5 ; Avg Loss 3.099148910673277\n",
            "Iteration 1870; Percent Complete 46.75 ; Avg Loss 3.0366500350935555\n",
            "Iteration 1880; Percent Complete 47.0 ; Avg Loss 3.0871946703849993\n",
            "Iteration 1890; Percent Complete 47.25 ; Avg Loss 3.1478746577969714\n",
            "Iteration 1900; Percent Complete 47.5 ; Avg Loss 3.1085466919126823\n",
            "Iteration 1910; Percent Complete 47.75 ; Avg Loss 3.084663234120694\n",
            "Iteration 1920; Percent Complete 48.0 ; Avg Loss 3.0167692838190563\n",
            "Iteration 1930; Percent Complete 48.25 ; Avg Loss 2.987891206232638\n",
            "Iteration 1940; Percent Complete 48.5 ; Avg Loss 3.0830149739895107\n",
            "Iteration 1950; Percent Complete 48.75 ; Avg Loss 3.058964156168244\n",
            "Iteration 1960; Percent Complete 49.0 ; Avg Loss 3.0820669520458126\n",
            "Iteration 1970; Percent Complete 49.25 ; Avg Loss 3.0664606149350293\n",
            "Iteration 1980; Percent Complete 49.5 ; Avg Loss 3.0582257748656674\n",
            "Iteration 1990; Percent Complete 49.75 ; Avg Loss 3.0658210618109067\n",
            "Iteration 2000; Percent Complete 50.0 ; Avg Loss 3.112733773287403\n",
            "skipping saving for now \n",
            "Iteration 2010; Percent Complete 50.24999999999999 ; Avg Loss 3.05495636537067\n",
            "Iteration 2020; Percent Complete 50.5 ; Avg Loss 3.109362070354632\n",
            "Iteration 2030; Percent Complete 50.74999999999999 ; Avg Loss 3.1048601090721943\n",
            "Iteration 2040; Percent Complete 51.0 ; Avg Loss 3.0192881343937805\n",
            "Iteration 2050; Percent Complete 51.24999999999999 ; Avg Loss 3.1013293835784745\n",
            "Iteration 2060; Percent Complete 51.5 ; Avg Loss 2.9690196053172646\n",
            "Iteration 2070; Percent Complete 51.74999999999999 ; Avg Loss 3.063859575790296\n",
            "Iteration 2080; Percent Complete 52.0 ; Avg Loss 2.979426574115681\n",
            "Iteration 2090; Percent Complete 52.25 ; Avg Loss 2.9888489573357786\n",
            "Iteration 2100; Percent Complete 52.5 ; Avg Loss 3.048308322268147\n",
            "Iteration 2110; Percent Complete 52.75 ; Avg Loss 3.0960291440558425\n",
            "Iteration 2120; Percent Complete 53.0 ; Avg Loss 3.016696496452553\n",
            "Iteration 2130; Percent Complete 53.25 ; Avg Loss 3.0556767924395416\n",
            "Iteration 2140; Percent Complete 53.5 ; Avg Loss 3.0229299234990954\n",
            "Iteration 2150; Percent Complete 53.75 ; Avg Loss 3.063933833473341\n",
            "Iteration 2160; Percent Complete 54.0 ; Avg Loss 3.0732483103537427\n",
            "Iteration 2170; Percent Complete 54.25 ; Avg Loss 3.0234600983901077\n",
            "Iteration 2180; Percent Complete 54.50000000000001 ; Avg Loss 3.059378660254481\n",
            "Iteration 2190; Percent Complete 54.75 ; Avg Loss 3.025727857182101\n",
            "Iteration 2200; Percent Complete 55.00000000000001 ; Avg Loss 3.0792689125887405\n",
            "Iteration 2210; Percent Complete 55.25 ; Avg Loss 3.048837948566926\n",
            "Iteration 2220; Percent Complete 55.50000000000001 ; Avg Loss 3.0810717247325448\n",
            "Iteration 2230; Percent Complete 55.75 ; Avg Loss 3.0500460938208285\n",
            "Iteration 2240; Percent Complete 56.00000000000001 ; Avg Loss 3.0600542171059564\n",
            "Iteration 2250; Percent Complete 56.25 ; Avg Loss 2.9977159327646037\n",
            "Iteration 2260; Percent Complete 56.49999999999999 ; Avg Loss 3.044445863071235\n",
            "Iteration 2270; Percent Complete 56.75 ; Avg Loss 3.086157939242292\n",
            "Iteration 2280; Percent Complete 56.99999999999999 ; Avg Loss 3.0088820553467865\n",
            "Iteration 2290; Percent Complete 57.25 ; Avg Loss 3.0402647895751116\n",
            "Iteration 2300; Percent Complete 57.49999999999999 ; Avg Loss 2.971559345087176\n",
            "Iteration 2310; Percent Complete 57.75 ; Avg Loss 3.0569794938031767\n",
            "Iteration 2320; Percent Complete 57.99999999999999 ; Avg Loss 3.0019526329495383\n",
            "Iteration 2330; Percent Complete 58.25 ; Avg Loss 3.066613088202837\n",
            "Iteration 2340; Percent Complete 58.5 ; Avg Loss 3.048189853455634\n",
            "Iteration 2350; Percent Complete 58.75 ; Avg Loss 2.977556202908096\n",
            "Iteration 2360; Percent Complete 59.0 ; Avg Loss 3.0385427187476544\n",
            "Iteration 2370; Percent Complete 59.25 ; Avg Loss 2.9820606843710507\n",
            "Iteration 2380; Percent Complete 59.5 ; Avg Loss 2.9967480778830753\n",
            "Iteration 2390; Percent Complete 59.75 ; Avg Loss 3.035165455794844\n",
            "Iteration 2400; Percent Complete 60.0 ; Avg Loss 3.0523671818788976\n",
            "Iteration 2410; Percent Complete 60.25 ; Avg Loss 2.9560419674430762\n",
            "Iteration 2420; Percent Complete 60.5 ; Avg Loss 3.0633100640497744\n",
            "Iteration 2430; Percent Complete 60.75000000000001 ; Avg Loss 3.0673703758688444\n",
            "Iteration 2440; Percent Complete 61.0 ; Avg Loss 3.0236553344476436\n",
            "Iteration 2450; Percent Complete 61.25000000000001 ; Avg Loss 3.055876324630067\n",
            "Iteration 2460; Percent Complete 61.5 ; Avg Loss 3.0719819252121874\n",
            "Iteration 2470; Percent Complete 61.75000000000001 ; Avg Loss 3.062307749000974\n",
            "Iteration 2480; Percent Complete 62.0 ; Avg Loss 2.992552957228424\n",
            "Iteration 2490; Percent Complete 62.25000000000001 ; Avg Loss 3.0825279685632907\n",
            "Iteration 2500; Percent Complete 62.5 ; Avg Loss 3.1123197964032476\n",
            "skipping saving for now \n",
            "Iteration 2510; Percent Complete 62.74999999999999 ; Avg Loss 3.060357749126056\n",
            "Iteration 2520; Percent Complete 63.0 ; Avg Loss 3.031215808158525\n",
            "Iteration 2530; Percent Complete 63.24999999999999 ; Avg Loss 2.983363465937037\n",
            "Iteration 2540; Percent Complete 63.5 ; Avg Loss 2.996188128869614\n",
            "Iteration 2550; Percent Complete 63.74999999999999 ; Avg Loss 3.06271298233251\n",
            "Iteration 2560; Percent Complete 64.0 ; Avg Loss 2.96216460593359\n",
            "Iteration 2570; Percent Complete 64.25 ; Avg Loss 3.030928234455453\n",
            "Iteration 2580; Percent Complete 64.5 ; Avg Loss 3.017614344038444\n",
            "Iteration 2590; Percent Complete 64.75 ; Avg Loss 2.9213605719252578\n",
            "Iteration 2600; Percent Complete 65.0 ; Avg Loss 3.019955442702061\n",
            "Iteration 2610; Percent Complete 65.25 ; Avg Loss 2.9660698049346386\n",
            "Iteration 2620; Percent Complete 65.5 ; Avg Loss 3.024860688616372\n",
            "Iteration 2630; Percent Complete 65.75 ; Avg Loss 2.94686261101888\n",
            "Iteration 2640; Percent Complete 66.0 ; Avg Loss 3.0374184628271466\n",
            "Iteration 2650; Percent Complete 66.25 ; Avg Loss 3.0234509768959557\n",
            "Iteration 2660; Percent Complete 66.5 ; Avg Loss 2.955830099287014\n",
            "Iteration 2670; Percent Complete 66.75 ; Avg Loss 3.002133114744239\n",
            "Iteration 2680; Percent Complete 67.0 ; Avg Loss 3.0173026509482095\n",
            "Iteration 2690; Percent Complete 67.25 ; Avg Loss 2.9884227436854895\n",
            "Iteration 2700; Percent Complete 67.5 ; Avg Loss 2.9126671625222698\n",
            "Iteration 2710; Percent Complete 67.75 ; Avg Loss 2.995803855733019\n",
            "Iteration 2720; Percent Complete 68.0 ; Avg Loss 3.010019280026821\n",
            "Iteration 2730; Percent Complete 68.25 ; Avg Loss 3.0398344647520856\n",
            "Iteration 2740; Percent Complete 68.5 ; Avg Loss 2.993668726395545\n",
            "Iteration 2750; Percent Complete 68.75 ; Avg Loss 2.976397252037726\n",
            "Iteration 2760; Percent Complete 69.0 ; Avg Loss 3.0276988589001537\n",
            "Iteration 2770; Percent Complete 69.25 ; Avg Loss 2.9219723880267727\n",
            "Iteration 2780; Percent Complete 69.5 ; Avg Loss 3.017796891942141\n",
            "Iteration 2790; Percent Complete 69.75 ; Avg Loss 2.9415919779995945\n",
            "Iteration 2800; Percent Complete 70.0 ; Avg Loss 3.023332774902811\n",
            "Iteration 2810; Percent Complete 70.25 ; Avg Loss 3.0172418209322793\n",
            "Iteration 2820; Percent Complete 70.5 ; Avg Loss 2.999521142805026\n",
            "Iteration 2830; Percent Complete 70.75 ; Avg Loss 2.999092020982046\n",
            "Iteration 2840; Percent Complete 71.0 ; Avg Loss 2.9567329012821633\n",
            "Iteration 2850; Percent Complete 71.25 ; Avg Loss 3.0618543104313667\n",
            "Iteration 2860; Percent Complete 71.5 ; Avg Loss 2.932367646122972\n",
            "Iteration 2870; Percent Complete 71.75 ; Avg Loss 2.9614043847784504\n",
            "Iteration 2880; Percent Complete 72.0 ; Avg Loss 2.981138809070545\n",
            "Iteration 2890; Percent Complete 72.25 ; Avg Loss 3.046976372681134\n",
            "Iteration 2900; Percent Complete 72.5 ; Avg Loss 2.939385087236815\n",
            "Iteration 2910; Percent Complete 72.75 ; Avg Loss 3.064977659994541\n",
            "Iteration 2920; Percent Complete 73.0 ; Avg Loss 2.9082767721633167\n",
            "Iteration 2930; Percent Complete 73.25 ; Avg Loss 2.979088602738392\n",
            "Iteration 2940; Percent Complete 73.5 ; Avg Loss 2.9727195599459035\n",
            "Iteration 2950; Percent Complete 73.75 ; Avg Loss 2.907139060769253\n",
            "Iteration 2960; Percent Complete 74.0 ; Avg Loss 2.9899735074596796\n",
            "Iteration 2970; Percent Complete 74.25 ; Avg Loss 2.9183030785305446\n",
            "Iteration 2980; Percent Complete 74.5 ; Avg Loss 2.9758096536327443\n",
            "Iteration 2990; Percent Complete 74.75 ; Avg Loss 2.989253905249583\n",
            "Iteration 3000; Percent Complete 75.0 ; Avg Loss 2.9241644543096954\n",
            "skipping saving for now \n",
            "Iteration 3010; Percent Complete 75.25 ; Avg Loss 3.046830692234524\n",
            "Iteration 3020; Percent Complete 75.5 ; Avg Loss 2.9462312879733465\n",
            "Iteration 3030; Percent Complete 75.75 ; Avg Loss 2.92253592784387\n",
            "Iteration 3040; Percent Complete 76.0 ; Avg Loss 2.9955549621275885\n",
            "Iteration 3050; Percent Complete 76.25 ; Avg Loss 3.0254290274271094\n",
            "Iteration 3060; Percent Complete 76.5 ; Avg Loss 2.9689245146372576\n",
            "Iteration 3070; Percent Complete 76.75 ; Avg Loss 2.9630941906667942\n",
            "Iteration 3080; Percent Complete 77.0 ; Avg Loss 2.9367421778513068\n",
            "Iteration 3090; Percent Complete 77.25 ; Avg Loss 2.948811667715988\n",
            "Iteration 3100; Percent Complete 77.5 ; Avg Loss 2.8938246283105893\n",
            "Iteration 3110; Percent Complete 77.75 ; Avg Loss 2.9550366865709017\n",
            "Iteration 3120; Percent Complete 78.0 ; Avg Loss 3.0350673578951413\n",
            "Iteration 3130; Percent Complete 78.25 ; Avg Loss 2.8751090570936464\n",
            "Iteration 3140; Percent Complete 78.5 ; Avg Loss 2.90222098802244\n",
            "Iteration 3150; Percent Complete 78.75 ; Avg Loss 2.91441075897356\n",
            "Iteration 3160; Percent Complete 79.0 ; Avg Loss 2.930721486841764\n",
            "Iteration 3170; Percent Complete 79.25 ; Avg Loss 2.8458523566052465\n",
            "Iteration 3180; Percent Complete 79.5 ; Avg Loss 2.889282333845064\n",
            "Iteration 3190; Percent Complete 79.75 ; Avg Loss 2.938154808092954\n",
            "Iteration 3200; Percent Complete 80.0 ; Avg Loss 2.8856524555069303\n",
            "Iteration 3210; Percent Complete 80.25 ; Avg Loss 2.842564897986659\n",
            "Iteration 3220; Percent Complete 80.5 ; Avg Loss 2.9112318776810975\n",
            "Iteration 3230; Percent Complete 80.75 ; Avg Loss 2.952566851797508\n",
            "Iteration 3240; Percent Complete 81.0 ; Avg Loss 2.8793204964381953\n",
            "Iteration 3250; Percent Complete 81.25 ; Avg Loss 3.0086150969170276\n",
            "Iteration 3260; Percent Complete 81.5 ; Avg Loss 2.8357347078049484\n",
            "Iteration 3270; Percent Complete 81.75 ; Avg Loss 2.9658437543058347\n",
            "Iteration 3280; Percent Complete 82.0 ; Avg Loss 3.0275032953122425\n",
            "Iteration 3290; Percent Complete 82.25 ; Avg Loss 2.8695290276794347\n",
            "Iteration 3300; Percent Complete 82.5 ; Avg Loss 2.9075264770574156\n",
            "Iteration 3310; Percent Complete 82.75 ; Avg Loss 2.9523003303117354\n",
            "Iteration 3320; Percent Complete 83.0 ; Avg Loss 2.935908916577204\n",
            "Iteration 3330; Percent Complete 83.25 ; Avg Loss 2.8963575000452666\n",
            "Iteration 3340; Percent Complete 83.5 ; Avg Loss 2.925258382219574\n",
            "Iteration 3350; Percent Complete 83.75 ; Avg Loss 2.876747700812316\n",
            "Iteration 3360; Percent Complete 84.0 ; Avg Loss 2.8940624771269747\n",
            "Iteration 3370; Percent Complete 84.25 ; Avg Loss 2.8178825434182055\n",
            "Iteration 3380; Percent Complete 84.5 ; Avg Loss 2.9697079971445746\n",
            "Iteration 3390; Percent Complete 84.75 ; Avg Loss 2.8989323048068103\n",
            "Iteration 3400; Percent Complete 85.0 ; Avg Loss 2.9286188071354236\n",
            "Iteration 3410; Percent Complete 85.25 ; Avg Loss 2.981843687981124\n",
            "Iteration 3420; Percent Complete 85.5 ; Avg Loss 2.9118145881013024\n",
            "Iteration 3430; Percent Complete 85.75 ; Avg Loss 2.957368083779859\n",
            "Iteration 3440; Percent Complete 86.0 ; Avg Loss 2.946974964027749\n",
            "Iteration 3450; Percent Complete 86.25 ; Avg Loss 2.929425006335944\n",
            "Iteration 3460; Percent Complete 86.5 ; Avg Loss 2.9602348979798867\n",
            "Iteration 3470; Percent Complete 86.75 ; Avg Loss 2.9368270994921586\n",
            "Iteration 3480; Percent Complete 87.0 ; Avg Loss 2.91988529171194\n",
            "Iteration 3490; Percent Complete 87.25 ; Avg Loss 2.8949625831102233\n",
            "Iteration 3500; Percent Complete 87.5 ; Avg Loss 2.8947560240238417\n",
            "skipping saving for now \n",
            "Iteration 3510; Percent Complete 87.75 ; Avg Loss 2.985840051850268\n",
            "Iteration 3520; Percent Complete 88.0 ; Avg Loss 2.9244567714888796\n",
            "Iteration 3530; Percent Complete 88.25 ; Avg Loss 2.8384104224206497\n",
            "Iteration 3540; Percent Complete 88.5 ; Avg Loss 2.975424996467712\n",
            "Iteration 3550; Percent Complete 88.75 ; Avg Loss 2.8773162244397383\n",
            "Iteration 3560; Percent Complete 89.0 ; Avg Loss 2.9007378962734798\n",
            "Iteration 3570; Percent Complete 89.25 ; Avg Loss 2.8840683595172565\n",
            "Iteration 3580; Percent Complete 89.5 ; Avg Loss 2.9220088378550813\n",
            "Iteration 3590; Percent Complete 89.75 ; Avg Loss 2.86122145404481\n",
            "Iteration 3600; Percent Complete 90.0 ; Avg Loss 2.9225869761684136\n",
            "Iteration 3610; Percent Complete 90.25 ; Avg Loss 2.928364252709618\n",
            "Iteration 3620; Percent Complete 90.5 ; Avg Loss 2.9089032311976792\n",
            "Iteration 3630; Percent Complete 90.75 ; Avg Loss 2.9104724268297013\n",
            "Iteration 3640; Percent Complete 91.0 ; Avg Loss 3.001568992501718\n",
            "Iteration 3650; Percent Complete 91.25 ; Avg Loss 2.828238537724818\n",
            "Iteration 3660; Percent Complete 91.5 ; Avg Loss 2.807713416976519\n",
            "Iteration 3670; Percent Complete 91.75 ; Avg Loss 2.8741375380476266\n",
            "Iteration 3680; Percent Complete 92.0 ; Avg Loss 2.833129529228712\n",
            "Iteration 3690; Percent Complete 92.25 ; Avg Loss 2.8581789135872055\n",
            "Iteration 3700; Percent Complete 92.5 ; Avg Loss 2.8756018243753587\n",
            "Iteration 3710; Percent Complete 92.75 ; Avg Loss 2.8652634359864786\n",
            "Iteration 3720; Percent Complete 93.0 ; Avg Loss 2.8396420909031423\n",
            "Iteration 3730; Percent Complete 93.25 ; Avg Loss 2.8955994254063526\n",
            "Iteration 3740; Percent Complete 93.5 ; Avg Loss 2.9483483247085394\n",
            "Iteration 3750; Percent Complete 93.75 ; Avg Loss 2.8941774447197672\n",
            "Iteration 3760; Percent Complete 94.0 ; Avg Loss 2.8413573123635603\n",
            "Iteration 3770; Percent Complete 94.25 ; Avg Loss 2.852624887929798\n",
            "Iteration 3780; Percent Complete 94.5 ; Avg Loss 2.868232470893899\n",
            "Iteration 3790; Percent Complete 94.75 ; Avg Loss 2.996190260958062\n",
            "Iteration 3800; Percent Complete 95.0 ; Avg Loss 2.8679867738308293\n",
            "Iteration 3810; Percent Complete 95.25 ; Avg Loss 2.958131099243025\n",
            "Iteration 3820; Percent Complete 95.5 ; Avg Loss 2.9186854472930994\n",
            "Iteration 3830; Percent Complete 95.75 ; Avg Loss 2.8028254023524104\n",
            "Iteration 3840; Percent Complete 96.0 ; Avg Loss 2.821323179473118\n",
            "Iteration 3850; Percent Complete 96.25 ; Avg Loss 2.8367766379845696\n",
            "Iteration 3860; Percent Complete 96.5 ; Avg Loss 2.7655216745513593\n",
            "Iteration 3870; Percent Complete 96.75 ; Avg Loss 2.904530474002997\n",
            "Iteration 3880; Percent Complete 97.0 ; Avg Loss 2.87275590201684\n",
            "Iteration 3890; Percent Complete 97.25 ; Avg Loss 2.819060621923951\n",
            "Iteration 3900; Percent Complete 97.5 ; Avg Loss 2.8458858940050336\n",
            "Iteration 3910; Percent Complete 97.75 ; Avg Loss 2.889038629119614\n",
            "Iteration 3920; Percent Complete 98.0 ; Avg Loss 2.821923623051207\n",
            "Iteration 3930; Percent Complete 98.25 ; Avg Loss 2.9021171989360903\n",
            "Iteration 3940; Percent Complete 98.5 ; Avg Loss 2.903748644006913\n",
            "Iteration 3950; Percent Complete 98.75 ; Avg Loss 2.8489063879960694\n",
            "Iteration 3960; Percent Complete 99.0 ; Avg Loss 2.847609390604341\n",
            "Iteration 3970; Percent Complete 99.25 ; Avg Loss 2.8510372827192265\n",
            "Iteration 3980; Percent Complete 99.5 ; Avg Loss 2.857977996542004\n",
            "Iteration 3990; Percent Complete 99.75 ; Avg Loss 2.8389688905835375\n",
            "Iteration 4000; Percent Complete 100.0 ; Avg Loss 2.8223303283530012\n",
            "skipping saving for now \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lZ4v-oY8dehD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "Ym6xKAzHi9_G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set dropout layers to eval\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder,decoder)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EVQbYAf1jNtw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "0c103aed-623a-490e-b625-771e7ce5316b"
      },
      "cell_type": "code",
      "source": [
        "# Begin chatting\n",
        "evaluateInput(encoder,decoder,searcher,voc)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> Hi model\n",
            "BOT : hi.\n",
            "> what the weather like\n",
            "BOT : yourenotgoingtodoit.\n",
            "> what am i \n",
            "BOT : yourenotgoingtodo.\n",
            "> ok\n",
            "BOT : imsorry.\n",
            "> for what\n",
            "BOT : thelittletime.\n",
            "> what little time \n",
            "BOT : imsorry.\n",
            "> it's ok\n",
            "BOT : what?\n",
            "> what is not ok\n",
            "BOT : yourenotgoingtodoit.\n",
            "> did you watch the movie\n",
            "BOT : imnot.\n",
            "> looks like you are stuck\n",
            "BOT : imsorry.\n",
            "> yup local mininma\n",
            "Error: encounterd unknown word\n",
            "> q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1RALSJi_jTYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e-M4p6znmDxa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f9aa3d7-394c-4ce0-de4b-6696aac69dd4"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NoneType: None\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "oh-PSpSNmGVf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}