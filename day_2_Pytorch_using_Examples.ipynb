{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_using_Examples.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "p0AJmjjQe1-k",
        "QmwjssdkgAoP",
        "MPlPLubAmce2",
        "GTQaOV0MbZuy",
        "VWer7TFo0_9l",
        "H4cB2UJq71uU",
        "Ev5370J0BBI5",
        "a4-8lRq4F8a4",
        "hiiSJemTGCpe"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "mWbgY76GTi8O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! pip3 install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0sKIiv8AWMuM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pytorch - n dimensional Tesor , can run on GPU ; Automatic differentiation \n",
        "\n",
        "Test Netwok - \n",
        "\n",
        "* simple fully connected Relu network  \n",
        "* One hidden layer \n",
        "* trained with gradient descent to fit random data by maximizing the Euclidean distance between the network output and true output\n",
        "\n",
        "* N - batch size\n",
        "* D_in - input dimension\n",
        "* H - hidden dimension\n",
        "* D_out - output dimension"
      ]
    },
    {
      "metadata": {
        "id": "kG6VYz71WbPj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensors "
      ]
    },
    {
      "metadata": {
        "id": "p0AJmjjQe1-k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Numpy network\n",
        "\n",
        "Before using Pytorch , creating a  network using numpy\n",
        "\n",
        "Manually implementing forward and backward passes "
      ]
    },
    {
      "metadata": {
        "id": "7iHoNA-2fHxd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "3d102afd-2666-423b-a654-0a29bcecc07a"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# N - batch size , D_in - input dimension , \n",
        "# H - Hidden dimension , D_out - output dimension\n",
        "N, D_in, H, D_out = 64,1000,100,10\n",
        "\n",
        "# Creating random input and output data\n",
        "x = np.random.randn(N,D_in)\n",
        "y = np.random.randn(N,D_out)\n",
        "\n",
        "# Randomly initializing weights\n",
        "w1 = np.random.randn(D_in,H)\n",
        "w2 = np.random.randn(H,D_out)\n",
        "\n",
        "# learning rate\n",
        "learning_rate =1e-6\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "for t in range(500):\n",
        "  # Forward Pass\n",
        "  h = x.dot(w1)\n",
        "  # relu squishes negetive value \n",
        "  h_relu = np.maximum(h,0)\n",
        "  y_pred = h_relu.dot(w2)\n",
        "  \n",
        "  # Computing and printing loss\n",
        "  loss = np.square(y_pred - y).sum()\n",
        "  if t%50==0:\n",
        "    print(t,loss)\n",
        "  \n",
        "  #Backprop\n",
        "  # d(x^2)/dx = 2x \n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  # h_relu.T - transpose of h_relu\n",
        "  # gradient of w2\n",
        "  grad_w2 = h_relu.T.dot(grad_y_pred)\n",
        "  # gradoent o.f hidden relu\n",
        "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "  # gradient of hidden unit\n",
        "  grad_h = grad_h_relu.copy()\n",
        "  # sqashing negetives\n",
        "  grad_h[h<0]=0\n",
        "  # gradient of w1\n",
        "  grad_w1 = x.T.dot(grad_h)\n",
        "  \n",
        "  #Updating the weights\n",
        "  w1 -= learning_rate*grad_w1\n",
        "  w2 -= learning_rate*grad_w2\n",
        "  \n",
        "  if t==499:\n",
        "    print('forward')\n",
        "    print(f'[{x.shape}] * [{w1.shape}] = [{h.shape}] = max(val,0) = [{h_relu.shape}] * [{w2.shape}] =[{y_pred.shape}]')\n",
        "    print('backward y\\' -> w2\\'(y\\',h) -> h\\'(y\\',w2) -> w1\\'(h\\',x)')\n",
        "    print(f'2 * ([{y_pred.shape}] - [{y.shape}]) = [{grad_y_pred.shape}]')\n",
        "    print(f'[{h_relu.T.shape}] * [{grad_y_pred.shape}] = [{grad_w2.shape}]')\n",
        "    print(f'[{grad_y_pred.shape}] * [{w2.T.shape}] = [{grad_h_relu.shape}]')\n",
        "    print(f'[{grad_h_relu.shape}] = [{grad_h.shape}]')\n",
        "    print(f'[{x.T.shape}] * [{grad_h.shape}] = [{grad_w1.shape}]')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 30219360.47258518\n",
            "50 18065.889810389395\n",
            "100 1271.6621803559697\n",
            "150 143.4248039489206\n",
            "200 18.4409501783745\n",
            "250 2.488475451111806\n",
            "300 0.34359649442843065\n",
            "350 0.04810594273538965\n",
            "400 0.006795415647632441\n",
            "450 0.000967698653036707\n",
            "forward\n",
            "[(64, 1000)] * [(1000, 100)] = [(64, 100)] = max(val,0) = [(64, 100)] * [(100, 10)] =[(64, 10)]\n",
            "backward y' -> w2'(y',h) -> h'(y',w2) -> w1'(h',x)\n",
            "2 * ([(64, 10)] - [(64, 10)]) = [(64, 10)]\n",
            "[(100, 64)] * [(64, 10)] = [(100, 10)]\n",
            "[(64, 10)] * [(10, 100)] = [(64, 100)]\n",
            "[(64, 100)] = [(64, 100)]\n",
            "[(1000, 64)] * [(64, 100)] = [(1000, 100)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QmwjssdkgAoP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tensors Network\n",
        "\n",
        "Tensors can be run on GPU and can keep track of computational graph and gradients \n",
        "\n",
        "Same network implemented using torch.tensors \n",
        "\n",
        "Manually implementing forward and backward pass "
      ]
    },
    {
      "metadata": {
        "id": "iGkXyD0vkJ0x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b3af73cf-0eb5-4565-f4cc-5ca47503c7d5"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# N - batch size , D_in - input dimension \n",
        "# H - hidden state , D_out - output dimension\n",
        "\n",
        "N, D_in, H, D_out = 64,1000,100,10\n",
        "\n",
        "# Creating random input and output using tensors\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initializing weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "learning_rate =1e-6\n",
        "\n",
        "for t in range(500):\n",
        "  # Forward Pass\n",
        "  # mm - matrix multiply\n",
        "  h = x.mm(w1)\n",
        "  # removing negetives , clamp all elements b/w [min,max] and return resulting tensor\n",
        "  h_relu = h.clamp(min=0)\n",
        "  # predictions \n",
        "  y_pred = h_relu.mm(w2)\n",
        "  \n",
        "  # Computing and printing loss\n",
        "  loss = (y_pred - y).pow(2).sum().item()\n",
        "  if t%50==0:\n",
        "    print(t,loss)\n",
        "    \n",
        "  # Backprop\n",
        "  # y'\n",
        "  grad_y_pred = 2.0 * (y_pred -y)\n",
        "  # w2'\n",
        "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "  # h'\n",
        "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "  grad_h = grad_h_relu.clone()\n",
        "  # squishing negetives\n",
        "  grad_h[h<0]=0\n",
        "  # w1'\n",
        "  grad_w1 = x.t().mm(grad_h)\n",
        "  \n",
        "  # Updating weights\n",
        "  w1 -= learning_rate * grad_w1\n",
        "  w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 25681834.0\n",
            "50 16346.099609375\n",
            "100 634.4251708984375\n",
            "150 42.14482879638672\n",
            "200 3.3945581912994385\n",
            "250 0.3015325665473938\n",
            "300 0.028702255338430405\n",
            "350 0.0031278894748538733\n",
            "400 0.0005423491238616407\n",
            "450 0.00016499737103004009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MPlPLubAmce2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## AutoGrad"
      ]
    },
    {
      "metadata": {
        "id": "nHytp6duQcoJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the previous networks we had to maually implement both forward and backward passes.\n",
        "\n",
        "We can use **automatic differentiation **  to automatically compute thhe backward pass gradients in the network\n",
        "\n",
        "Autograd uses ** Computational graph ** which is defined in the forward pass . Nodes in the graph will be tensors and edges will be functions that produce output tensor from input tensors . Backpropagating through this graph then allows for easy computation of gradients\n",
        "\n",
        "Practice - Each tensor representes a node in a computational graph . If **x** is a tensor that has **x.requires_grad=True** , then **x.grad** is another tensor hplding the **gradient of tensor x w.r.t  some scalar value **"
      ]
    },
    {
      "metadata": {
        "id": "A14C57UgZ5c2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "f9e38ce6-40c5-4d3b-9265-e1cd833b3d8c"
      },
      "cell_type": "code",
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# batch size , in,hidden,out params \n",
        "N, D_in, H, D_out = 64,1000,100,10\n",
        "\n",
        "# Creating random input and outputs\n",
        "# Setting requires_grad=False (default) indicates that gradients \n",
        "# need not be computed during backward pass\n",
        "x = torch.randn(N,D_in,dtype=dtype,device=device)\n",
        "y = torch.randn(N,D_out,dtype=dtype,device=device)\n",
        "\n",
        "# Creating random weights \n",
        "# Setting requires_grad=True to ensure that gradients are calculated \n",
        "# w.r.t these tensors during backward pass\n",
        "w1 = torch.randn(D_in, H, dtype=dtype,device=device,requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, dtype=dtype,device=device,requires_grad=True)\n",
        "\n",
        "learning_rate =1e-6\n",
        "\n",
        "for t in range(500):\n",
        "  # Forward Pass\n",
        "  # Same operations but as gradients are not being calculated manually\n",
        "  # for backward pass , intermediate variables can be ignored \n",
        "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "  \n",
        "  # Computing and printing loss \n",
        "  # loss is a tensor of shape 1 , use item() to  get value\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "  if t%50==0:\n",
        "    print(t,loss.item())\n",
        "  \n",
        "  # Using autograd to calculate the gradients \n",
        "  # This call will compute the gradient of loss w.r.t all tensors with \n",
        "  # requires_grad=True \n",
        "  # After this call w1.grad, w2.grad will be tensors holding the \n",
        "  # gradient of the loss w.r.t w1 and w2 respectively\n",
        "  loss.backward()\n",
        "  \n",
        "  # Manually updating weights using gradient descent. \n",
        "  # Wrapping in torch.no_grad() as weights have requires_grad=True but don't \n",
        "  # need to track this operation in autograd\n",
        "  \n",
        "  # Alternative way - operate on weight.data and weight.grad.data \n",
        "  # tensor.data gives a tensor that shares the storage with the the tensor\n",
        "  # but does not track history\n",
        "  # Also can use torch.optim.SGD to achieve tis \n",
        "  with torch.no_grad():\n",
        "    w1 -= learning_rate * w1.grad\n",
        "    w2 -= learning_rate * w2.grad\n",
        "    \n",
        "    # Manually setting gradients after updating weights\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()\n",
        "  \n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 32710250.0\n",
            "50 22505.185546875\n",
            "100 1221.1851806640625\n",
            "150 105.53775024414062\n",
            "200 11.311941146850586\n",
            "250 1.3562647104263306\n",
            "300 0.17225566506385803\n",
            "350 0.022683413699269295\n",
            "400 0.003314564935863018\n",
            "450 0.0006640014471486211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GTQaOV0MbZuy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining new Autograd Functions \n",
        "\n",
        "Each primitive autograd operator is just two functions that operate on tensors .\n",
        "\n",
        "**Forward -** computes the output tensors from input tensors .\n",
        "\n",
        "** Backward -** recieves the gradient of the output tensor w.r.t some scalar value and computes the gradient of the input tensor w.r.t  to the same scalar value \n",
        "\n",
        "Can easily define our own autograd operator by defining a subclass of **torch.autograd.Function** and implement **forward** and **backward** functions.\n",
        "Can use the new autograd operator by constructing an instance and calling it , passing tensors containing input data."
      ]
    },
    {
      "metadata": {
        "id": "58DPyFEcwua4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating a custom Relu autograd function"
      ]
    },
    {
      "metadata": {
        "id": "JYXlWBmhuWlu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyRelU(torch.autograd.Function):\n",
        "  \"\"\"\n",
        "  Can implement our own custom autograd by \n",
        "  defining forward and backward passes\n",
        "  which operate on tensors\n",
        "  \"\"\"\n",
        "  \n",
        "  @staticmethod\n",
        "  def forward(ctx,input):\n",
        "    \"\"\"\n",
        "    In the forward pass we recieve a tensor containing the input \n",
        "    and return a Tensor containing the output. ctx is a context \n",
        "    object that can be used to stash info for backward computation.\n",
        "    Can save arbitrary objects for use in the backward pass using\n",
        "    ctx.save_for_backward method\n",
        "    \"\"\"\n",
        "    ctx.save_for_backward(input)\n",
        "    return input.clamp(min=0)\n",
        "  \n",
        "  @staticmethod\n",
        "  def backward(ctx,grad_output):\n",
        "    \"\"\"\n",
        "    In the backward pass we recieve a tensor containing the gradient\n",
        "    of loss w.r.t the output , and we need to compute the gradient\n",
        "    of loss w.r.t input \n",
        "    \"\"\"\n",
        "    \n",
        "    input, = ctx.saved_tensors\n",
        "    grad_input = grad_output.clone()\n",
        "    # setting the gradient =0 for the values of input\n",
        "    # where value =0\n",
        "    grad_input[input<0]=0\n",
        "    return grad_input "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pgZCBnunxsNs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "83ce9b7b-1c3a-40ec-ac67-6ca2ce437f32"
      },
      "cell_type": "code",
      "source": [
        "# Creating the same two layer network\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# Batch size , in,hidden, out params\n",
        "N,D_in,H,D_out = 64,1000,100,10\n",
        "\n",
        "# random input and outputs\n",
        "x = torch.randn(N,D_in,dtype=dtype,device=device)\n",
        "y = torch.randn(N,D_out,dtype=dtype,device=device)\n",
        "\n",
        "# random weights \n",
        "w1 = torch.randn(D_in,H,dtype=dtype,device=device,requires_grad=True)\n",
        "w2 = torch.randn(H,D_out,dtype=dtype,device=device,requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "for t in range(500):\n",
        "  # To apply own function , using .apply method\n",
        "  relu = MyRelU.apply\n",
        "  \n",
        "  # Forward Pass \n",
        "  y_pred = relu(x.mm(w1)).mm(w2)\n",
        "  \n",
        "  # Computing loss and print\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "  if t%50==0:\n",
        "    print(t,loss.item())\n",
        "  \n",
        "  # Using autograd for backward pass\n",
        "  loss.backward()\n",
        "  \n",
        "  # updating weights using gradient descent\n",
        "  with torch.no_grad():\n",
        "    w1 -= learning_rate * w1.grad\n",
        "    w2 -= learning_rate * w2.grad\n",
        "    \n",
        "    # Manually setting the gradients to zerro\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()\n",
        "    \n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 31186850.0\n",
            "50 17259.037109375\n",
            "100 998.8756713867188\n",
            "150 93.18627166748047\n",
            "200 10.428625106811523\n",
            "250 1.2847579717636108\n",
            "300 0.16770946979522705\n",
            "350 0.0229497030377388\n",
            "400 0.003501452738419175\n",
            "450 0.0007342651369981468\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VWer7TFo0_9l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## TensorFlow : Static Graphs"
      ]
    },
    {
      "metadata": {
        "id": "1qvZvJsZ1KrO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pytorch and tensorflow both define a computational graph and use\n",
        "automatic differentition to compute gradients .\n",
        "\n",
        "**Difference ** \n",
        "    \n",
        "* Tensorflow the computational graphs are **static**\n",
        "            \n",
        "*  Pytorch computational graphs are **dynamic**\n",
        "\n",
        "In Tensorflow we define the computational graph once and then execute the same graph over and over again , possibly feeding different inpuut data to the graph.\n",
        "\n",
        "In Pytorch , each forward pass defines a new computational graph\n",
        "\n",
        "**Static Graphs ** - advantage , can be optimized upfront ; framework meight decide to fuse some graph operations for efficiency or come up with a strategy for distributing the graph across many GPU's . \n",
        "\n",
        "If reusing the same graph over and over again => the potentially costly optimization can be amortized and same graph can be resused again and again.\n",
        "\n",
        "** Control Flow **\n",
        "\n",
        "some models we may wish to perform different computation for each data point.\n",
        "\n",
        "Eg - RNN - unrolled for different number of timesteps for each data point which can be implemented in a loop . \n",
        "\n",
        "With **Static graph ** the loop construct needs to be a part of the graph. ( The reason Tensorflow provides **tf.scan** operator for embedding loops into the graph .)\n",
        "\n",
        "With **Dynamic graphs** the situation is simpler: since we build the graph on-the-fly for each example , we can use normal imperative flow control to perform computation that differs for each input."
      ]
    },
    {
      "metadata": {
        "id": "cDgruyvF5SFo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using Tensorflow to fit the same network"
      ]
    },
    {
      "metadata": {
        "id": "P14s0O9Z5oEa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "de5fe748-4f09-4e63-b88e-b9c4b2f8b15c"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# First we set up the computational graph\n",
        "\n",
        "# batch size , in , hidden , out params\n",
        "N,D_in,H,D_out = 64,1000,100,10\n",
        "\n",
        "# Create placeholders for the input  and output data \n",
        "# These will be filled with real data when the graph is executed\n",
        "\n",
        "x = tf.placeholder(tf.float32,shape=(None,D_in))\n",
        "y = tf.placeholder(tf.float32,shape=(None,D_out))\n",
        "\n",
        "# Create variables for the weights and initialize them with random data\n",
        "# A tensorflow variable persists its value across executions of graph\n",
        "w1 = tf.Variable(tf.random_normal((D_in,H)))\n",
        "w2 = tf.Variable(tf.random_normal((H,D_out)))\n",
        "\n",
        "# Forward Pass \n",
        "# Compute the predicted y using operations on Tensorflow tesnors\n",
        "# This code does not perform any numerical operations , it merely\n",
        "# sets up the computational graph that will later execute\n",
        "h = tf.matmul(x,w1)\n",
        "h_relu = tf.maximum(h,tf.zeros(1))\n",
        "y_pred = tf.matmul(h_relu,w2)\n",
        "\n",
        "# Computing loass using operations on Tensorflow tensors\n",
        "# Reduces `input_tensor` along the dimensions given in `axis`.\n",
        "loss = tf.reduce_sum((y-y_pred)**2)\n",
        "\n",
        "# Compute the gradient of loss w.r.t w1 and w2\n",
        "# Constructs symbolic derivatives of sum of `ys` w.r.t. x in `xs`\n",
        "grad_w1,grad_w2 = tf.gradients(loss, [w1,w2])\n",
        "\n",
        "# Update weights using gradient descent\n",
        "# To actually update the weights , we need to evaluate new_w1 and new_w2 \n",
        "# when executing the graph \n",
        "# In Tensorflow the act of updating the value of weights is part of \n",
        "# the computational graph\n",
        "# In Pyttorch this happens outside the computational graph\n",
        "learning_rate =1e-6\n",
        "\n",
        "new_w1 = w1.assign(w1 - learning_rate * grad_w1)\n",
        "new_w2 = w2.assign(w2 - learning_rate * grad_w2)\n",
        "\n",
        "# Now the computational graph is built, enter a tensorflow session \n",
        "# to actually execute the graph\n",
        "with tf.Session() as sess:\n",
        "  # Run the graph once to initialize the weight variable w1,w2\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  # Create numpy arrays holding the actual data for x,y\n",
        "  x_value = np.random.randn(N,D_in)\n",
        "  y_value = np.random.randn(N,D_out)\n",
        "  \n",
        "  for t in range(500):\n",
        "    # Execute the graph many times , each time it executes we want \n",
        "    # to bind x_value to x and y_value to y, specified with feed_dict\n",
        "    # Each time we execute the graph we want to compute the values for\n",
        "    # loss , new_w1,new_w2 ; The values of these tensors are returned\n",
        "    # as numpy arrays\n",
        "    loss_value,_,_ = sess.run([loss,new_w1,new_w2],\n",
        "                             feed_dict={x:x_value,y:y_value})\n",
        "    if t%50==0:\n",
        "      print(t,loss_value)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 32104396.0\n",
            "50 12914.943\n",
            "100 476.44772\n",
            "150 27.931505\n",
            "200 1.9787308\n",
            "250 0.15744771\n",
            "300 0.0137233585\n",
            "350 0.0015263855\n",
            "400 0.00030967587\n",
            "450 0.00010701763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SrGz9OX-6PDW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## nn.Module"
      ]
    },
    {
      "metadata": {
        "id": "H4cB2UJq71uU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pytorch:nn"
      ]
    },
    {
      "metadata": {
        "id": "stjuggQG8EAk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Autograd and computational graphs can be too low level\n",
        "\n",
        "When building of Neural Networks think of arranging the computation into **layers** ,\n",
        "which have learnable parameters which will be optimized during learning\n",
        "\n",
        "**nn** package defines a set of **Modules** whih is roughly equivalent to neural  network layer\n",
        "\n",
        "A module recieves an input tensor and computes output tesnsor ,but may also hold internal state such as tensors containing learnable parameter \n"
      ]
    },
    {
      "metadata": {
        "id": "jctXr1xe8lY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "6e3e432b-48e5-4dbd-ba25-490a8206d4c0"
      },
      "cell_type": "code",
      "source": [
        "# batch size , in,hidden,out params\n",
        "N,D_in,H,D_out = 64,1000,100,10\n",
        "\n",
        "# Create random input and output tensors\n",
        "x = torch.randn(N,D_in)\n",
        "y = torch.randn(N,D_out)\n",
        "\n",
        "# Use the nn package to define outr model as a sequence of layers .\n",
        "# nn.Sequential is a Module which cotains other modules , and \n",
        "# applies them in a sequence to produce its output.\n",
        "# Each Linear Module computes output from input using a linear function\n",
        "# and holds internal tensors for its weight and bias\n",
        "model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(D_in,H),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(H,D_out))\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions \n",
        "# Using MSE as the loss function\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate=1e-4\n",
        "\n",
        "for t in range(500):\n",
        "  # Forward Pass \n",
        "  # compute predicted y by passing input x to the model.\n",
        "  # Module object override the __call__ operator so they can be called \n",
        "  # like functions .\n",
        "  # Pass a tensor as input and get a tensor of output data\n",
        "  y_pred = model(x)\n",
        "  \n",
        "  # Compute and print loss \n",
        "  # Pass tensors containing predicted and true values of y\n",
        "  # loss function returns tensor containing loss \n",
        "  loss = loss_fn(y_pred,y)\n",
        "  if t%50==0:\n",
        "    print(t,loss.item())\n",
        "  \n",
        "  # Manually setting the gradients as zero\n",
        "  model.zero_grad()\n",
        "  \n",
        "  # Backward Pass \n",
        "  # Compute gradient od the loss w.r.t all the learnable parms\n",
        "  # internally the parameters of each Module are stored in tensors \n",
        "  # with reuires_grad=True\n",
        "  # Computing gradients for all learnable params\n",
        "  loss.backward()\n",
        "  \n",
        "  # Update the weights using gradeint descent \n",
        "  with torch.no_grad():\n",
        "    for param in model.parameters():\n",
        "      param -= learning_rate*param.grad"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 679.7244873046875\n",
            "50 27.946258544921875\n",
            "100 1.8075411319732666\n",
            "150 0.21464239060878754\n",
            "200 0.03270195797085762\n",
            "250 0.005752671975642443\n",
            "300 0.0011209604563191533\n",
            "350 0.00023694874835200608\n",
            "400 5.314098598319106e-05\n",
            "450 1.245086059498135e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ev5370J0BBI5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pytorch:optim"
      ]
    },
    {
      "metadata": {
        "id": "FfdRpjVXEeRJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So far updating weights manually using torch.no_grad() or .data  to avoid tracking history in autograd\n",
        "\n",
        "Becomes complex for complex optimizers like AdaGrad, RMSProp, Adam\n",
        "\n",
        "**optim** package in Pytorch abstracts the idea and provides implementation of commonly used optimization algos."
      ]
    },
    {
      "metadata": {
        "id": "t5LULCdpF39j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "7e4d4c14-8b8c-4239-db52-ebd0ff4a4804"
      },
      "cell_type": "code",
      "source": [
        "# batch size , in, hidden m out params\n",
        "N, D_in, H, D_out =64, 1000, 100, 10\n",
        "\n",
        "# Creating random x and y \n",
        "x = torch.randn(N,D_in)\n",
        "y = torch.randn(N,D_out)\n",
        "\n",
        "# Using nn Sequntial package to create the model\n",
        "model = torch.nn.Sequential(\n",
        "        torch.nn.Linear(D_in,H),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.Linear(H,D_out))\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "# Using the optim package to define an Optimizer that will update the weights\n",
        "# Using Adam optimizer here - the first argument to Adam constructor tells the \n",
        "# optimizer which tenors to update\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "for t in range(500):\n",
        "  # Forward Pass , computing predicted_y using x and model\n",
        "  y_pred = model(x)\n",
        "  \n",
        "  # Computing and printing loss\n",
        "  loss = loss_fn(y_pred,y)\n",
        "  if t%50==0:\n",
        "    print(t,loss.item())\n",
        "    \n",
        "  # Before backward pass , use optimizer to zero all gradients for the variables\n",
        "  # it will update( which are the learnable weights of the model). This is because\n",
        "  # by default gradients are accumulated in buffers (i.e. not overwritten) whenever\n",
        "  # backwards is called . \n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # Backward Pass , compute gradient of the loss w.r.t model params\n",
        "  loss.backward()\n",
        "  \n",
        "  # Calling the step function on an Optimizer makes an update to its params\n",
        "  optimizer.step()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 714.4263916015625\n",
            "50 223.8377227783203\n",
            "100 55.706539154052734\n",
            "150 8.764324188232422\n",
            "200 1.0608502626419067\n",
            "250 0.1430402547121048\n",
            "300 0.017975587397813797\n",
            "350 0.0017298394814133644\n",
            "400 0.00013976407353766263\n",
            "450 8.440280907962006e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a4-8lRq4F8a4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pytorch:Custom nn Modules"
      ]
    },
    {
      "metadata": {
        "id": "9PyZaFoCp25Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sometimes models can be more complex than a sequence of existing Modules, For these can define own Modules by subclassing **nn.Module** and defining   **forward** method - which recieves input tensors and produces output tensor using other modules or other autograd operations on tensors"
      ]
    },
    {
      "metadata": {
        "id": "mETckaPdGB18",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TwoLayerNN(torch.nn.Module):\n",
        "  def __init__(self,D_in,H,D_out):\n",
        "    \"\"\"\n",
        "    In constructor instantiating two nn.Linear modules and assigning them as \n",
        "    member variables\n",
        "    \"\"\"\n",
        "    super(TwoLayerNN,self).__init__()\n",
        "    \n",
        "    self.l1 = torch.nn.Linear(D_in,H)\n",
        "    self.l2 = torch.nn.Linear(H,D_out)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    \"\"\"\n",
        "    In forward function we accept a Tensor of input data and we must return a \n",
        "    Tensor of output data. We can use Modules defined in the constructor as well\n",
        "    as arbitrary operators on Tensors\n",
        "    \"\"\"\n",
        "    h_relu = self.l1(x).clamp(min=0)\n",
        "    y_pred = self.l2(h_relu)\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IvFjFtiMrw_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "1d975f5c-485b-4e16-b3d0-a9daaec2da6a"
      },
      "cell_type": "code",
      "source": [
        "# batch size , in ,hidden, out params\n",
        "N,D_in,H,D_out = 64,1000,100,10\n",
        "\n",
        "#Creating random x and y \n",
        "x = torch.randn(N,D_in)\n",
        "y = torch.randn(N,D_out)\n",
        "\n",
        "# Creating model\n",
        "model = TwoLayerNN(D_in,H,D_out)\n",
        "\n",
        "learning_rate =1e-4\n",
        "\n",
        "# Creating a loss function and an optimizer \n",
        "# model.parameters() will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are memebers of the model\n",
        "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
        "# passing the params to SGD optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "for t in range(500):\n",
        "  # Forward Pass\n",
        "  y_pred = model(x)\n",
        "  \n",
        "  # Calculating and printing loss\n",
        "  loss = criterion(y_pred,y)\n",
        "  if t%50==0:\n",
        "    print(t,loss.item())\n",
        "    \n",
        "  # Zero gradients ,backward pass , update weights\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 629.7481689453125\n",
            "50 28.894826889038086\n",
            "100 1.7199313640594482\n",
            "150 0.18932399153709412\n",
            "200 0.02804916352033615\n",
            "250 0.005026581697165966\n",
            "300 0.0010181789984926581\n",
            "350 0.00022453453857451677\n",
            "400 5.2563187637133524e-05\n",
            "450 1.2877923836640548e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hiiSJemTGCpe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pytorch:Control Flow + Weight Sharing"
      ]
    },
    {
      "metadata": {
        "id": "XyNMg9M5y390",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating a dynamic network - Fully connected ReLU network \n",
        "  \n",
        "on each forward pass chooses a random number b/w 1 and 4 => that many hidden layers. Reusing the same weights multiple times to cimmpute the innermost hidden layers\n",
        "\n",
        "Implement using loop and reusing the same Module multiple times when defining forward pass"
      ]
    },
    {
      "metadata": {
        "id": "jO8yRQBUGIgJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class DynamicNN(torch.nn.Module):\n",
        "  def __init__(self,D_in,H,D_out):\n",
        "    \"\"\"\n",
        "    In constructor defining the three nn.Linear instaances that will be \n",
        "    used in the forward pass\n",
        "    \"\"\"\n",
        "    super(DynamicNN,self).__init__()\n",
        "    self.input_layer = torch.nn.Linear(D_in,H)\n",
        "    self.middle_layer = torch.nn.Linear(H,H)\n",
        "    self.output_layer = torch.nn.Linear(H,D_out)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    \"\"\"\n",
        "    For forward pass , randomly choose either 0,1,2,3 and reuse the middle_layer\n",
        "    module that many times to compute hidden layer computations\n",
        "    \n",
        "    Each Forward pass builds a dynamic computational graph , thus can use normal\n",
        "    pythonic control statements -loop,if/else when defining forward pass of model\n",
        "    \"\"\"\n",
        "    h_relu = self.input_layer(x).clamp(min=0)\n",
        "    for _ in range(random.randint(0,3)):\n",
        "      h_relu = self.middle_layer(h_relu).clamp(min=0)\n",
        "    \n",
        "    y_pred = self.output_layer(h_relu)\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cs4SNCQ83b8d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "fcbdcb49-3aa6-4db9-f153-78f3f728c1ff"
      },
      "cell_type": "code",
      "source": [
        "# batch size , in , hidden , out params \n",
        "N,D_in,H,D_out = 64,1000,100,10\n",
        "\n",
        "# Creating random x and y\n",
        "x = torch.randn(N,D_in)\n",
        "y = torch.randn(N,D_out)\n",
        "\n",
        "# Creating the model\n",
        "model = DynamicNN(D_in,H,D_out)\n",
        "\n",
        "# Optimizer and loss function \n",
        "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
        "# Using momentum here as training can be slow\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=1e-4,momentum=0.9)\n",
        "\n",
        "for t in range(500):\n",
        "  # Forward Pass \n",
        "  y_pred = model(x)\n",
        "  \n",
        "  # Computing and printing loss\n",
        "  loss = criterion(y_pred,y)\n",
        "  if t%50==0:\n",
        "    print(t,loss.item())\n",
        "    \n",
        "  # Zero gradients , backward pass , update weights\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 656.40625\n",
            "50 159.1815643310547\n",
            "100 22.87323570251465\n",
            "150 5.997773170471191\n",
            "200 0.7470123767852783\n",
            "250 1.5068737268447876\n",
            "300 2.8965821266174316\n",
            "350 0.7964998483657837\n",
            "400 0.4362240135669708\n",
            "450 5.319315433502197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s1vYPrBd4qfJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}