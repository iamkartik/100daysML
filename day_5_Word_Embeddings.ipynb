{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embeddings.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "e23xNTgpGCsS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word Embeddings - Encoding Lexical Semantics\n"
      ]
    },
    {
      "metadata": {
        "id": "p22lHVVQ--Ox",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Word Embeddings ** are dense vectors of real numbers , one per word in the vocabulary. \n",
        "\n",
        "Representing text , words is a challenge in NLP as need to store it's meaning as well.  \n",
        "\n",
        "Also given input is $ |V|$ dimensional ( where V is our vocabulary ) we want an output with only a few dimensions .( Need to move from higer dimension to lower one)\n",
        "\n",
        "** One Hot Encoding ** we can represent a word by \n",
        "$$ \\overbrace {\\left [0,0,\\dots,1,\\dots,0,0\\right]} ^\\text {|V| elements} $$\n",
        "\n",
        "where 1 is in a location unique to word w . Every word will have a 1 in a unique location and zeros everywhere else\n",
        "\n",
        "**Problem** this representation treats all words as **independent entities ** with no relation to each other . We need a notion of **Similarity** between words.\n",
        "i.e **Semantic Similarity ** and not just orthographic representations. Using this technique we can combat the **sparsity ** of linguistic data by connecting the dots of what we have seen and what we haven't. Eg\n",
        "\n",
        "* Mathematican ran to the store. (Train)\n",
        "* Physicist ran to the store.(Train)\n",
        "* Mathematician solved the open problem.(Train)\n",
        "\n",
        "* Physicist solved the open problem. (Test)\n",
        "\n",
        "Using Symantic similarity the n/w can genralize this sentence . The eg relies on a fundamental linguistic assumprion; words appering in similar contexts are related to each other. - **Distributional Hypothesis**"
      ]
    },
    {
      "metadata": {
        "id": "Wlcr7TV9W8Pf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Getting Dense Word Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "GgHB1Ln9YnN-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Encoding Semantic Similarity in words -  Make up some semantic attributes and give scores to them , common words can have similar score .\n",
        "\n",
        "$$ q_\\text{mathematician} = \\left [\\overbrace{2.3}^\\text{can run},\\overbrace{9.1}^\\text{likes coffee},\\overbrace{-5.5}^\\text{majored in Physics},\\dots\\right]$$\n",
        "\n",
        "\n",
        "$$ q_\\text{physicist} = \\left [\\overbrace{2.3}^\\text{can run},\\overbrace{9.1}^\\text{likes coffee},\\overbrace{9.1}^\\text{majored in Physics},\\dots\\right]$$\n",
        "\n",
        "Then the measure of similarity can be  = $ q_\\text{mathematician} \\cdot q_\\text{physicist} $\n",
        "\n",
        "After normalization Similarity ( physicist,mathematician  ) = $$ \\frac {q_\\text{mathematician} \\cdot q_\\text{physicist}} {||q_\\text{mathematician} || || q_\\text{physicist}||} = cos(\\phi) $$\n",
        "\n",
        "\n",
        "Where $\\phi$ is the angle between the two vectors . Extrmely similar words ( where embeddings point in the same direction) will have similarity =1 , Dissimilar words will have similarity =-1\n",
        "\n",
        "One Hot Encoding - special case where each word basically has similarity =0 , and each word is given a unique semantic attribute. Whereas the one hot encoded vectors are sparse , these vectors are dense with entries typically non-zero.\n",
        "\n",
        "\n",
        "Using the neural network to learn the representations i.e ** keeping the word embedddings as parameters in the model and learning and updating them during training ** Note - The learnt word embeddings will not be interpretable .\n",
        "\n",
        "** Word Embeddings are a representation of the SEMANTICS of a word -> efficiently encoding semantic information that might be relevant to the task at hand **\n"
      ]
    },
    {
      "metadata": {
        "id": "Sdx9GLQ0UHnw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word Embeddings in Pytorch "
      ]
    },
    {
      "metadata": {
        "id": "Wcs67W2HUOt8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Need to define an indexx for each word when using embeddings \n",
        "\n",
        "Embeddings are stored as $ |V| \\times D $  matrix , where D is the dimensionality of the embeddings, such that the word assigned index i has it's embeddings stored in the i'th row of the matrix \n",
        "\n",
        "** torch.nn.Embedding - takes two  arguments : vocab size , dimensionallity of the embeddings **"
      ]
    },
    {
      "metadata": {
        "id": "Q4HexQ78VRKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "6d32a029-c920-45ac-f7b4-3105f6d08dfd"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 32kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x592c6000 @  0x7f705c0071c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BVyCzF44VeXM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gw1XTZkpVm0v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "284e2632-2392-467a-becb-3d04bf80006c"
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbfe90bd190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "pxRWh5ZOVpqi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "02774922-0327-402f-950c-a9be689fae0e"
      },
      "cell_type": "code",
      "source": [
        "# indexing the words \n",
        "words_to_ix = {\"hello\":0,\"world\":1}\n",
        "# 2 words in the vocab - hello , world\n",
        "# dimensionality of embeddings = 5\n",
        "embeddings = nn.Embedding(2,5)\n",
        "# getting the index of word hello and converting to a long tensor\n",
        "lookup_tensor = torch.tensor([words_to_ix[\"hello\"]], dtype=torch.long)\n",
        "# getting the embeddding at index 0 -> hello \n",
        "hello_embedding = embeddings(lookup_tensor)\n",
        "print(lookup_tensor)\n",
        "print(hello_embedding)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0])\n",
            "tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]], grad_fn=<EmbeddingBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cbA9o0E8WkGp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### N Gram Modelling\n",
        "\n",
        "Given a sequence of words w , compute\n",
        "\n",
        "$$ P(w_i | w_{i-1}, w_{i-2},\\dots,w_{i-n+1}) $$\n",
        "\n",
        "where $ w_i $ is the i'th word of the sequence \n",
        "\n",
        "Computing the loss function and updating params with backprop "
      ]
    },
    {
      "metadata": {
        "id": "N--zMF9jXoUD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b0532b9-0395-4955-f78a-3e0c667f643f"
      },
      "cell_type": "code",
      "source": [
        "# Context size is the size of window to look for context\n",
        "# 2 words to left - asymmetric window\n",
        "CONTEXT_SIZE =2 \n",
        "EMBEDDING_DIM = 10\n",
        "\n",
        "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
        "And dig deep trenches in thy beauty's field,\n",
        "Thy youth's proud livery so gazed on now,\n",
        "Will be a totter'd weed of small worth held:\n",
        "Then being asked, where all thy beauty lies,\n",
        "Where all the treasure of thy lusty days;\n",
        "To say, within thine own deep sunken eyes,\n",
        "Were an all-eating shame, and thriftless praise.\n",
        "How much more praise deserv'd thy beauty's use,\n",
        "If thou couldst answer 'This fair child of mine\n",
        "Shall sum my count, and make my old excuse,'\n",
        "Proving his beauty by succession thine!\n",
        "This were to be new made when thou art old,\n",
        "And see thy blood warm when thou feel'st it cold.\"\"\"\n",
        "\n",
        "test_sentence = test_sentence.split()\n",
        "\n",
        "# Tokenizing the input \n",
        "# Building a list of tuples , Each tuple is ([word_i-2,word_i-1],target word)\n",
        "\n",
        "trigrams = [(\n",
        "            [test_sentence[i],test_sentence[i+1]],\n",
        "            test_sentence[i+2]) \n",
        "            for i in range(len(test_sentence)-2) ]\n",
        "\n",
        "print(trigrams[:3])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-6BBcqgubykx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set removing duplicates\n",
        "vocab = set(test_sentence)\n",
        "\n",
        "word_to_ix = {word:i for i,word in enumerate(vocab)}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pHW6X5srcIsn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NGramLangClassifier(nn.Module):\n",
        "  def __init__(self,vocab_size,dimensions,context_size):\n",
        "    super(NGramLangClassifier,self).__init__()\n",
        "    # The Embedding Layer\n",
        "    self.embeddings =  nn.Embedding(vocab_size,dimensions)\n",
        "    # Linear layer\n",
        "    self.linear1 = nn.Linear(context_size*dimensions,128)\n",
        "    # second layer/output layer will give probability for each word\n",
        "    self.linear2 = nn.Linear(128,vocab_size)\n",
        "    \n",
        "  def forward(self,inputs):\n",
        "    embedding = self.embeddings(inputs).view((1,-1))\n",
        "    out1 = F.relu(self.linear1(embedding))\n",
        "    out2 = self.linear2(out1)\n",
        "    # returning the log_probabilities of each word\n",
        "    return F.log_softmax(out2,dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1vcwCdA_d4dh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# loss function , optimizer and model\n",
        "losses =[]\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "model = NGramLangClassifier(len(vocab),EMBEDDING_DIM,CONTEXT_SIZE)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lj8Iprq7eRYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "16b45a26-02c5-4ea0-fca5-9c35eaf3dd4d"
      },
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "for epoch in range(10):\n",
        "  total_loss =0\n",
        "  for context,target in trigrams:\n",
        "    # 1. Get the word indexes for the trigrams \n",
        "    context_idx = torch.tensor([word_to_ix[w] for w in context],dtype=torch.long)\n",
        "    target_idx = torch.tensor([word_to_ix[target]],dtype=torch.long)\n",
        "    \n",
        "    #2. Zero the gradients being accumulated\n",
        "    model.zero_grad()\n",
        "    \n",
        "    # 3. Forward Pass\n",
        "    log_probs = model(context_idx)\n",
        "    \n",
        "    # 4. Compute loss\n",
        "    loss = loss_function(log_probs,target_idx)\n",
        "    \n",
        "    # 5. Backprop and updating gradients\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # appending the losses\n",
        "    total_loss +=loss.item()\n",
        "  losses.append(total_loss)\n",
        "\n",
        "print(losses)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[524.9456593990326, 522.3348758220673, 519.7448451519012, 517.1742217540741, 514.6207783222198, 512.0832452774048, 509.56009459495544, 507.05187797546387, 504.55892968177795, 502.07942605018616]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "O8kl81Qrfug0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Computing Word Embeddings : Continuous Bag-Of-Words"
      ]
    },
    {
      "metadata": {
        "id": "7a9oQIpjf4ky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Continuous Bag-Of-Words (CBOW) is frequently used in NLP\n",
        "\n",
        "Model tries to predict words given the context of a few words before and and few words after the target word. \n",
        "\n",
        "Distinct from language modelling -> ** CBOW is not sequential and does not have to be probabilistic **\n",
        "\n",
        "CBOW used to quickly train word embeddings --> these embeddings used to initialize the embeddings of some more complicated model ** Pretrained Embedding **\n",
        "\n",
        "** CBOW Model ** given a target word $w_i$ and an $ N $ context window on each size - $ w_{i-1},\\dots,w_{i-N} and w_{i+1},\\dots,w_{i+N}  $ referring to all context words as $ C $ , CBOW tries to minimize \n",
        "\n",
        "$$ -log p(w_i | C ) = -log Softmax (A (\\sum_{w \\in C} q_w) +b ) $$\n",
        "\n",
        "\n",
        "where $ q_w $ is the embedding for the word w "
      ]
    },
    {
      "metadata": {
        "id": "zNoRDGstka9b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3ca2a2c1-03f5-4dea-867d-0ad404ff2c84"
      },
      "cell_type": "code",
      "source": [
        "# Context size - symmetrical 2 to left , 2 to right\n",
        "CONTEXT_SIZE = 2 \n",
        "\n",
        "text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\"\n",
        "\n",
        "text = text.split()\n",
        "\n",
        "# removing duplicates\n",
        "vocab = set(text)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word_to_ix =  {word:i for i,word in enumerate(vocab)}\n",
        "\n",
        "# creating the sets of context , target\n",
        "data =[]\n",
        "\n",
        "for i in range(2, len(text) - 2):\n",
        "  context =[text[i - 2],text[i - 1],text[i+1],text[i+2]]\n",
        "  target = text[i]\n",
        "  data.append((context,target))\n",
        "  \n",
        "\n",
        "print(data[:4])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fAecTV8smUM2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b2ca92d6-a9ff-4569-f501-1c58d14d87b7"
      },
      "cell_type": "code",
      "source": [
        "# function to return indexes for a given context \n",
        "def make_context_vector(context,word_to_ix):\n",
        "  idxs = [word_to_ix[w] for w in context]\n",
        "  return torch.tensor(idxs,dtype=torch.long)\n",
        "\n",
        "make_context_vector(data[0][0],word_to_ix)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([17, 21, 16, 43])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "_vDyhq05nUtK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "ead28e04-6aed-4507-f4d8-7fee35f8ec67"
      },
      "cell_type": "code",
      "source": [
        "# CBOW Model\n",
        "# class CBOW(nn.Module):\n",
        "#   def __init__(self,vocab_size,embedding_dim,context_size):\n",
        "#     super(CBOW,self).__init__()\n",
        "#     # Embedding Layer\n",
        "#     self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "#     # layer 1\n",
        "#     self.l1 = nn.Linear(embedding_dim * context_size,128)\n",
        "#     # layer 2\n",
        "#     self.l2 = nn.Linear(128,vocab_size)\n",
        "    \n",
        "#   def forward(self,inputs):\n",
        "#     embed = self.embedding(inputs).view(1,-1)\n",
        "#     o1 = F.relu(self.l1(embed))\n",
        "#     o2 = self.l2(o1)\n",
        "#     return F.log_softmax(o2,dim=1)\n",
        "\n",
        "\n",
        "\n",
        "class CBOW2(nn.Module):\n",
        "  def __init__(self,vocab_size,embedding_dim,context_size):\n",
        "    super(CBOW2,self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "    self.l1 = nn.Linear(embedding_dim,vocab_size)\n",
        "\n",
        "    \n",
        "  def forward(self,inputs):\n",
        "    embed = self.embedding(inputs).sum(dim=0).view((1,-1))\n",
        "    out = self.l1(embed)\n",
        "    return F.log_softmax(out,dim=1)\n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-166a6333ca5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mCBOW2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCBOW2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "yzSsQgxqO-rB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bjpW9uwFO-0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-sJBF6bfooFV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "losses=[]\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "\n",
        "model = CBOW(vocab_size,EMBEDDING_DIM,2*CONTEXT_SIZE)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "71w7ju66o8M-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "17c09526-b44b-437b-c16c-010750e0b42f"
      },
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "for epoch in range(10):\n",
        "  total_loss=0\n",
        "  for context, target in data:\n",
        "    #1. Calculate the indexs\n",
        "    context_idx = make_context_vector(context,word_to_ix)\n",
        "    target_idx = make_context_vector([target],word_to_ix)\n",
        "    \n",
        "    #2. Zero the gradients\n",
        "    model.zero_grad()\n",
        "    \n",
        "    #3. forward pass\n",
        "    log_prob = model(context_idx)\n",
        "    \n",
        "    #4. loss calculation\n",
        "    loss = loss_function(log_prob,target_idx)\n",
        "    \n",
        "    #5. Backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    total_loss += loss.item()\n",
        "  \n",
        "  losses.append(total_loss)\n",
        "  \n",
        "print(losses)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[227.04092955589294, 225.45482993125916, 223.88139843940735, 222.3219611644745, 220.77350521087646, 219.2351851463318, 217.70639371871948, 216.18591237068176, 214.67090392112732, 213.1623682975769]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dkwR6WuIqnMa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}