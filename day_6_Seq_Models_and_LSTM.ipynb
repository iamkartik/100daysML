{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq Models and LSTM.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vqHonPk7PKoM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sequence Models and Long-Short Term Memory Networks"
      ]
    },
    {
      "metadata": {
        "id": "q_G_LTG_zeCO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Simple Feed Forward Networks - No state maintained by the network\n",
        "\n",
        "**Sequence Models** - Some sort of dependence through time between the inputs . Eg - Hidden Markov Models for oart of speech tagging , Conditional Random Field\n",
        "\n",
        "**Reccurent Neural Network ** - network that maintains some kind of state. Eg output used as a part of next input => information can propagate along the network as it passes throug the sequence.\n",
        "\n",
        "** LSTM ** - for each element in the sequence there is a corresponding hidden state $h_t$ , which in principle can contain information about arbitrary points earlier in the sequence . We can use the hidden state to predict words in a language model , part of speech tags etc.\n"
      ]
    },
    {
      "metadata": {
        "id": "fxXPmSengYC0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### LSTM's in Pytorch \n",
        "\n",
        "\n",
        "* Pytorch LSTM's expects all it's inputs  to  be  3D tensors.\n",
        "* First axis is the sequence itself\n",
        "* Second axis indexes instances in the mini-batch\n",
        "* Third axis indexes elements of the input\n",
        "\n",
        "Using mini-batch = 1 for now => have just 1 dimension on the second axis\n",
        "\n",
        "Eg - \" The cow jumped\"\n",
        "\n",
        "$$ input = \\begin{split} \\begin{bmatrix}  \n",
        "\\overbrace {q_\\text{The}^\\text{row vector}} \\\\\n",
        "q_\\text{cow}\\\\\n",
        "q_\\text{jumped} \\\\\n",
        "\\end{bmatrix}\\end{split}$$\n",
        "\n",
        "ALsi going through the sequence 1 at a time => have just 1 dimension at the first axis"
      ]
    },
    {
      "metadata": {
        "id": "JVx6LRrvii31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "209358af-ac22-4636-e543-82883dd6b733"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/0e/e382bcf1a6ae8225f50b99cc26effa2d4cc6d66975ccf3fa9590efcbedce/torch-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (519.5MB)\n",
            "\u001b[K    100% |████████████████████████████████| 519.5MB 28kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x5a25a000 @  0x7f45cadbd1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "deLAifGVil0f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f3EItSH9iztQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2eb23480-e182-4dc4-a743-73bd3fb6a807"
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f24bd0d9070>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "NdNUWFZzi4u6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "c0eb0218-9859-4b38-a95f-b859b8d3a114"
      },
      "cell_type": "code",
      "source": [
        "# LSTM input dimension is 3 o/p dimension is 3\n",
        "lstm = nn.LSTM(3,3)\n",
        "# a random sequence of length 5\n",
        "inputs = [torch.randn(1,3) for _ in range(5)]\n",
        "\n",
        "# initializing the hidden state\n",
        "hidden = (torch.randn(1,1,3),\n",
        "          torch.randn(1,1,3))\n",
        "\n",
        "print(inputs)\n",
        "print(\"\\n\")\n",
        "for i in inputs:\n",
        "  # Stepping through the sequence one element at a time \n",
        "  # after each step, hidden contains the hidden state\n",
        "  out, hidden = lstm(i.view(1,1,-1),hidden)\n",
        "  print(i,out,hidden)\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor([[-0.8996,  0.5313,  0.4034]]), tensor([[ 1.4521, -2.4182, -1.1906]]), tensor([[0.6964, 1.1296, 0.2214]]), tensor([[-0.0558,  1.2057,  1.9486]]), tensor([[-0.0766, -0.8562, -0.7870]])]\n",
            "\n",
            "\n",
            "tensor([[-0.8996,  0.5313,  0.4034]]) tensor([[[ 0.0774, -0.1934,  0.0383]]], grad_fn=<CatBackward>) (tensor([[[ 0.0774, -0.1934,  0.0383]]], grad_fn=<ViewBackward>), tensor([[[ 0.1577, -0.9874,  0.0760]]], grad_fn=<ViewBackward>))\n",
            "\n",
            "\n",
            "tensor([[ 1.4521, -2.4182, -1.1906]]) tensor([[[-0.0414, -0.2163, -0.2406]]], grad_fn=<CatBackward>) (tensor([[[-0.0414, -0.2163, -0.2406]]], grad_fn=<ViewBackward>), tensor([[[-0.1984, -0.4736, -0.4137]]], grad_fn=<ViewBackward>))\n",
            "\n",
            "\n",
            "tensor([[0.6964, 1.1296, 0.2214]]) tensor([[[-0.0332, -0.0876, -0.1996]]], grad_fn=<CatBackward>) (tensor([[[-0.0332, -0.0876, -0.1996]]], grad_fn=<ViewBackward>), tensor([[[-0.1296, -0.1687, -0.4776]]], grad_fn=<ViewBackward>))\n",
            "\n",
            "\n",
            "tensor([[-0.0558,  1.2057,  1.9486]]) tensor([[[ 0.1046, -0.1599, -0.0232]]], grad_fn=<CatBackward>) (tensor([[[ 0.1046, -0.1599, -0.0232]]], grad_fn=<ViewBackward>), tensor([[[ 0.2208, -0.3843, -0.0619]]], grad_fn=<ViewBackward>))\n",
            "\n",
            "\n",
            "tensor([[-0.0766, -0.8562, -0.7870]]) tensor([[[-0.0515, -0.0881, -0.0360]]], grad_fn=<CatBackward>) (tensor([[[-0.0515, -0.0881, -0.0360]]], grad_fn=<ViewBackward>), tensor([[[-0.1480, -0.2065, -0.0630]]], grad_fn=<ViewBackward>))\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mDr835vNj0xk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "56f5d8a5-dcc9-432e-8611-fdcdcc5254a5"
      },
      "cell_type": "code",
      "source": [
        "# Also can do the entire sequence at once\n",
        "# The first value returned by LSTM (out) is all of the hidden states throughout\n",
        "# the sequence\n",
        "# The second (hidden) is just the most recent hidden state \n",
        "# \"out\" will allow access to all the hidden states in the sequence\n",
        "# \"hidden\" will allow to continue the sequence and bakpropagate\n",
        "# by passing it as an argument to the lstm at a later time \n",
        "\n",
        "# adding the extra 2nd dimension\n",
        "inputs = torch.cat(inputs).view(len(inputs),1,-1)\n",
        "print(inputs)\n",
        "# cleaning the hidden state\n",
        "hidden = (torch.randn(1,1,3),\n",
        "          torch.randn(1,1,3))\n",
        "\n",
        "out , hidden = lstm(inputs,hidden)\n",
        "\n",
        "print(out)\n",
        "print(\"\\n\")\n",
        "print(hidden)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.8996,  0.5313,  0.4034]],\n",
            "\n",
            "        [[ 1.4521, -2.4182, -1.1906]],\n",
            "\n",
            "        [[ 0.6964,  1.1296,  0.2214]],\n",
            "\n",
            "        [[-0.0558,  1.2057,  1.9486]],\n",
            "\n",
            "        [[-0.0766, -0.8562, -0.7870]]])\n",
            "tensor([[[-0.0376, -0.0257,  0.3951]],\n",
            "\n",
            "        [[-0.0604,  0.0050,  0.1696]],\n",
            "\n",
            "        [[-0.0310, -0.0039, -0.0257]],\n",
            "\n",
            "        [[ 0.1014, -0.1452,  0.0700]],\n",
            "\n",
            "        [[-0.0491, -0.0798,  0.0753]]], grad_fn=<CatBackward>)\n",
            "\n",
            "\n",
            "(tensor([[[-0.0491, -0.0798,  0.0753]]], grad_fn=<ViewBackward>), tensor([[[-0.1431, -0.1807,  0.1322]]], grad_fn=<ViewBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jzUA21TOljqE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### LSTM for part-of-speech tagging"
      ]
    },
    {
      "metadata": {
        "id": "7H53gbZgmIJJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Model :  \n",
        "\n",
        "* Input - $ w_1,\\dots,w_m ,$ where $w_i\\in V $ ( the vocab)\n",
        "* T - tag set and $y_i$, the tag of word $w_i$\n",
        "* Denote the prediction of $w_i$ as $\\hat{y_i}$\n",
        "* Output - a sequence of $ \\hat{y_1},\\dots,\\hat{y_m}$ , where $\\hat{y_i} \\in T $\n",
        "\n",
        "To do the prediction, pass an LSTM over the sentence . Denote the hidden state at timestamp $ i $ as $h_i$ .\n",
        "\n",
        "Assign each tag a unique index ( similar to word_to_ix) , then the prediction rule for $\\hat{y_i}$ is \n",
        "\n",
        "$$ \\hat{y_i} = argmax_j(log Softmax(Ah_i +b))_j $$\n",
        "\n",
        "i.e take the log softmax of the affine map of the hidden state , and the predicted tag is the tag that has the maximum value in this vector. \n",
        "\n",
        "** Note -  this implies that the dimensionality of the target space of A is |T| "
      ]
    },
    {
      "metadata": {
        "id": "KwrOHNYWs9zw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1164d6e1-26ab-481d-d9bb-9edf26bb31de"
      },
      "cell_type": "code",
      "source": [
        "# Preparing data\n",
        "def prepare_sequence(seq,to_ix):\n",
        "  idxs = [ to_ix[w] for w in seq]\n",
        "  return torch.tensor(idxs,dtype=torch.long)\n",
        "\n",
        "training_data = [\n",
        "    (\"The dog ate the apple\".split(),[\"DET\",\"NN\",\"V\",\"DET\",\"NN\"]),\n",
        "    (\"Everybody read that book\".split(),[\"NN\",\"V\",\"DET\",\"NN\"])\n",
        "]\n",
        "\n",
        "word_to_ix={}\n",
        "\n",
        "for sentence,tags in training_data:\n",
        "  for word in sentence:\n",
        "    if word not in word_to_ix:\n",
        "      word_to_ix[word]=len(word_to_ix)\n",
        "\n",
        "      \n",
        "print(word_to_ix)\n",
        "\n",
        "tag_to_ix = {\"V\":0,\"NN\":1,\"DET\":2}\n",
        "\n",
        "# Keeping embedding and hidden dimensions small to see how training progresses\n",
        "EMBEDDING_DIM =6\n",
        "HIDDEN_DIM = 6"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SO7u3EsGuX84",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The model class\n",
        "\n",
        "class LSTMTag(nn.Module):\n",
        "  def __init__(self,embedding_dim,hidden_dim,vocab_size,tagset_size):\n",
        "    super(LSTMTag,self).__init__()\n",
        "    \n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.embeddings = nn.Embedding(vocab_size,embedding_dim)\n",
        "    \n",
        "    # The LSTM takes word embeddings as inputs and outputs hidden states\n",
        "    # with dimensionality hidden_dim\n",
        "    self.lstm = nn.LSTM(embedding_dim,hidden_dim)\n",
        "    \n",
        "    # The linear layer that maps from hidden state to tag space\n",
        "    self.hidden2tag = nn.Linear(hidden_dim,tagset_size)\n",
        "    # Creating a default hidden state\n",
        "    self.hidden = self.init_hidden()\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    # Num layers , minibatch_size , hidden_dim)\n",
        "    return (torch.zeros(1,1,self.hidden_dim),\n",
        "           torch.zeros(1,1,self.hidden_dim))\n",
        "  \n",
        "  def forward(self,sentence):\n",
        "    embeds = self.embeddings(sentence)\n",
        "    \n",
        "    lstm_out,self.hidden = self.lstm(embeds.view(len(sentence),1,-1),self.hidden)\n",
        "    \n",
        "    tag_space = self.hidden2tag(lstm_out.view(len(sentence),-1))\n",
        "    \n",
        "    tag_scores = F.log_softmax(tag_space,dim=1)\n",
        "    \n",
        "    return tag_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XDmew8AEwres",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training The Model\n",
        "\n",
        "model = LSTMTag(EMBEDDING_DIM,HIDDEN_DIM,len(word_to_ix),len(tag_to_ix))\n",
        "\n",
        "loss_funtion = nn.NLLLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(),lr=0.1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EJ-gHJzJz-Kf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9f795338-90c5-46fa-e588-83f5be50db59"
      },
      "cell_type": "code",
      "source": [
        "# Checking outputs before training\n",
        "with torch.no_grad():\n",
        "  inputs = prepare_sequence(training_data[0][0],word_to_ix)\n",
        "  tag_scores = model(inputs)\n",
        "  print(tag_scores)\n",
        "  \n",
        "# Element i,j of the output is the score for tag (V,NN,DET) j for word i\n",
        "# sentence \"The dog ate the apple\" -> [\"DET\",\"NN\",\"V\",\"DET\",\"NN\"]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.1889, -0.9245, -1.2083],\n",
            "        [-1.1635, -0.9058, -1.2608],\n",
            "        [-1.0403, -0.9671, -1.3224],\n",
            "        [-1.0702, -0.9456, -1.3145],\n",
            "        [-1.0898, -0.9396, -1.2985]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gx9ODx2T0ZVD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10217
        },
        "outputId": "61fd8162-1655-49f3-d246-42b6c9494b37"
      },
      "cell_type": "code",
      "source": [
        "# Training Loop \n",
        "for epoch in range(300):\n",
        "  for sentence, tags in training_data:\n",
        "    # 1. Get the indexes for input and target\n",
        "    input_ix = prepare_sequence(sentence,word_to_ix)\n",
        "    target_ix = prepare_sequence(tags,tag_to_ix)\n",
        "    \n",
        "    #2. Zero the gradients as they get accumulated every step\n",
        "    model.zero_grad()\n",
        "    \n",
        "    #3.** Clear ot the hidden state of LSTM, detaching it from the \n",
        "    # history on last instance **\n",
        "    model.hidden = model.init_hidden()\n",
        "    \n",
        "    # 4. Forward Pass\n",
        "    target_score = model(input_ix)\n",
        "    \n",
        "    # 5. Calculate Loss\n",
        "    loss = loss_funtion(target_score,target_ix)\n",
        "    print(loss.item())\n",
        "    \n",
        "    #6. Backprop and udating weights\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0816985368728638\n",
            "1.030313491821289\n",
            "1.0767052173614502\n",
            "1.0258985757827759\n",
            "1.072096586227417\n",
            "1.0217320919036865\n",
            "1.0678132772445679\n",
            "1.0177655220031738\n",
            "1.0638043880462646\n",
            "1.0139576196670532\n",
            "1.0600260496139526\n",
            "1.0102732181549072\n",
            "1.056441068649292\n",
            "1.0066817998886108\n",
            "1.0530167818069458\n",
            "1.0031577348709106\n",
            "1.0497251749038696\n",
            "0.999678373336792\n",
            "1.0465415716171265\n",
            "0.9962244033813477\n",
            "1.0434446334838867\n",
            "0.992779016494751\n",
            "1.0404155254364014\n",
            "0.9893273711204529\n",
            "1.0374375581741333\n",
            "0.9858566522598267\n",
            "1.0344960689544678\n",
            "0.9823555946350098\n",
            "1.0315775871276855\n",
            "0.9788143038749695\n",
            "1.0286707878112793\n",
            "0.9752240777015686\n",
            "1.0257647037506104\n",
            "0.9715774655342102\n",
            "1.0228502750396729\n",
            "0.9678676128387451\n",
            "1.0199183225631714\n",
            "0.9640887975692749\n",
            "1.0169614553451538\n",
            "0.9602357149124146\n",
            "1.013972520828247\n",
            "0.9563041925430298\n",
            "1.010945200920105\n",
            "0.9522902965545654\n",
            "1.00787353515625\n",
            "0.9481906294822693\n",
            "1.004752278327942\n",
            "0.9440027475357056\n",
            "1.0015761852264404\n",
            "0.9397245645523071\n",
            "0.9983412623405457\n",
            "0.9353538751602173\n",
            "0.9950432777404785\n",
            "0.9308897256851196\n",
            "0.9916784167289734\n",
            "0.9263312220573425\n",
            "0.9882436990737915\n",
            "0.9216777086257935\n",
            "0.9847357869148254\n",
            "0.9169292449951172\n",
            "0.9811522364616394\n",
            "0.9120859503746033\n",
            "0.9774904251098633\n",
            "0.9071483612060547\n",
            "0.9737483859062195\n",
            "0.9021177291870117\n",
            "0.96992427110672\n",
            "0.8969948887825012\n",
            "0.966016411781311\n",
            "0.8917815089225769\n",
            "0.9620233774185181\n",
            "0.8864794373512268\n",
            "0.9579440951347351\n",
            "0.8810907602310181\n",
            "0.9537776708602905\n",
            "0.8756178021430969\n",
            "0.9495234489440918\n",
            "0.8700628280639648\n",
            "0.9451805949211121\n",
            "0.8644291162490845\n",
            "0.9407490491867065\n",
            "0.8587192296981812\n",
            "0.9362286329269409\n",
            "0.8529366254806519\n",
            "0.9316191673278809\n",
            "0.8470844626426697\n",
            "0.9269208908081055\n",
            "0.8411664366722107\n",
            "0.9221340417861938\n",
            "0.8351859450340271\n",
            "0.9172590374946594\n",
            "0.8291470408439636\n",
            "0.91229647397995\n",
            "0.8230533599853516\n",
            "0.9072467684745789\n",
            "0.8169088363647461\n",
            "0.9021108746528625\n",
            "0.8107175827026367\n",
            "0.8968895673751831\n",
            "0.8044835329055786\n",
            "0.8915837407112122\n",
            "0.7982109189033508\n",
            "0.8861943483352661\n",
            "0.7919036746025085\n",
            "0.8807225227355957\n",
            "0.7855658531188965\n",
            "0.8751692771911621\n",
            "0.7792016267776489\n",
            "0.8695356249809265\n",
            "0.7728148102760315\n",
            "0.8638229370117188\n",
            "0.7664093971252441\n",
            "0.8580324053764343\n",
            "0.7599892616271973\n",
            "0.8521652221679688\n",
            "0.7535581588745117\n",
            "0.8462226986885071\n",
            "0.7471193671226501\n",
            "0.8402062654495239\n",
            "0.7406766414642334\n",
            "0.8341167569160461\n",
            "0.7342332601547241\n",
            "0.8279557228088379\n",
            "0.7277923822402954\n",
            "0.8217247128486633\n",
            "0.7213569283485413\n",
            "0.8154246211051941\n",
            "0.7149298787117004\n",
            "0.8090570569038391\n",
            "0.7085137367248535\n",
            "0.8026229739189148\n",
            "0.702111005783081\n",
            "0.7961239218711853\n",
            "0.6957240104675293\n",
            "0.7895611524581909\n",
            "0.689354658126831\n",
            "0.7829355001449585\n",
            "0.6830048561096191\n",
            "0.776248574256897\n",
            "0.6766761541366577\n",
            "0.7695013880729675\n",
            "0.6703699827194214\n",
            "0.7626949548721313\n",
            "0.6640875339508057\n",
            "0.7558304071426392\n",
            "0.6578301191329956\n",
            "0.7489088773727417\n",
            "0.6515980958938599\n",
            "0.741931140422821\n",
            "0.645392119884491\n",
            "0.7348982095718384\n",
            "0.6392126083374023\n",
            "0.7278109788894653\n",
            "0.6330596208572388\n",
            "0.720670223236084\n",
            "0.6269330978393555\n",
            "0.7134766578674316\n",
            "0.6208330392837524\n",
            "0.7062309384346008\n",
            "0.6147584319114685\n",
            "0.6989337205886841\n",
            "0.6087090969085693\n",
            "0.6915857791900635\n",
            "0.6026840209960938\n",
            "0.6841875314712524\n",
            "0.5966821908950806\n",
            "0.676739513874054\n",
            "0.590702474117279\n",
            "0.6692423224449158\n",
            "0.5847432613372803\n",
            "0.6616965532302856\n",
            "0.5788034200668335\n",
            "0.6541029214859009\n",
            "0.5728809833526611\n",
            "0.6464616656303406\n",
            "0.5669743418693542\n",
            "0.6387738585472107\n",
            "0.5610817074775696\n",
            "0.6310400366783142\n",
            "0.5552009344100952\n",
            "0.6232611536979675\n",
            "0.5493301153182983\n",
            "0.6154382228851318\n",
            "0.543467104434967\n",
            "0.6075723767280579\n",
            "0.5376098155975342\n",
            "0.599664568901062\n",
            "0.5317561030387878\n",
            "0.5917167663574219\n",
            "0.5259039402008057\n",
            "0.5837303400039673\n",
            "0.5200511813163757\n",
            "0.5757070183753967\n",
            "0.5141959190368652\n",
            "0.5676490664482117\n",
            "0.508336067199707\n",
            "0.5595585703849792\n",
            "0.5024700164794922\n",
            "0.5514382123947144\n",
            "0.4965958893299103\n",
            "0.543290913105011\n",
            "0.4907122850418091\n",
            "0.5351191759109497\n",
            "0.4848179817199707\n",
            "0.5269266366958618\n",
            "0.478911817073822\n",
            "0.5187166333198547\n",
            "0.47299280762672424\n",
            "0.5104930996894836\n",
            "0.4670603573322296\n",
            "0.5022598505020142\n",
            "0.4611141085624695\n",
            "0.49402111768722534\n",
            "0.45515385270118713\n",
            "0.4857814908027649\n",
            "0.4491797685623169\n",
            "0.4775456488132477\n",
            "0.4431922733783722\n",
            "0.46931833028793335\n",
            "0.43719199299812317\n",
            "0.4611048698425293\n",
            "0.4311799108982086\n",
            "0.45291024446487427\n",
            "0.4251573085784912\n",
            "0.4447399973869324\n",
            "0.4191254675388336\n",
            "0.4365996718406677\n",
            "0.4130863547325134\n",
            "0.4284948706626892\n",
            "0.4070418179035187\n",
            "0.4204312264919281\n",
            "0.4009939432144165\n",
            "0.41241455078125\n",
            "0.3949452042579651\n",
            "0.40445050597190857\n",
            "0.38889801502227783\n",
            "0.39654484391212463\n",
            "0.38285505771636963\n",
            "0.38870319724082947\n",
            "0.3768192231655121\n",
            "0.38093122839927673\n",
            "0.3707934021949768\n",
            "0.37323421239852905\n",
            "0.36478060483932495\n",
            "0.3656176030635834\n",
            "0.35878369212150574\n",
            "0.3580862879753113\n",
            "0.3528060019016266\n",
            "0.35064539313316345\n",
            "0.3468506932258606\n",
            "0.34329938888549805\n",
            "0.34092089533805847\n",
            "0.3360525965690613\n",
            "0.3350197672843933\n",
            "0.32890909910202026\n",
            "0.32915064692497253\n",
            "0.3218727111816406\n",
            "0.32331663370132446\n",
            "0.3149469494819641\n",
            "0.31752100586891174\n",
            "0.3081347346305847\n",
            "0.31176671385765076\n",
            "0.30143895745277405\n",
            "0.3060570955276489\n",
            "0.29486197233200073\n",
            "0.30039528012275696\n",
            "0.2884059250354767\n",
            "0.29478415846824646\n",
            "0.28207239508628845\n",
            "0.2892268896102905\n",
            "0.27586302161216736\n",
            "0.28372663259506226\n",
            "0.2697787880897522\n",
            "0.27828606963157654\n",
            "0.2638203501701355\n",
            "0.27290812134742737\n",
            "0.2579883337020874\n",
            "0.2675955891609192\n",
            "0.2522827982902527\n",
            "0.2623511552810669\n",
            "0.24670377373695374\n",
            "0.2571775019168854\n",
            "0.2412506639957428\n",
            "0.25207698345184326\n",
            "0.23592320084571838\n",
            "0.24705222249031067\n",
            "0.23072032630443573\n",
            "0.2421051263809204\n",
            "0.22564108669757843\n",
            "0.23723778128623962\n",
            "0.2206842601299286\n",
            "0.23245230317115784\n",
            "0.21584859490394592\n",
            "0.22774991393089294\n",
            "0.21113237738609314\n",
            "0.22313255071640015\n",
            "0.20653405785560608\n",
            "0.218601256608963\n",
            "0.2020517885684967\n",
            "0.2141570746898651\n",
            "0.1976836621761322\n",
            "0.20980089902877808\n",
            "0.19342784583568573\n",
            "0.20553326606750488\n",
            "0.18928198516368866\n",
            "0.20135459303855896\n",
            "0.18524427711963654\n",
            "0.19726508855819702\n",
            "0.1813121736049652\n",
            "0.19326463341712952\n",
            "0.17748384177684784\n",
            "0.1893528401851654\n",
            "0.1737566739320755\n",
            "0.1855296790599823\n",
            "0.17012862861156464\n",
            "0.1817941665649414\n",
            "0.16659724712371826\n",
            "0.17814570665359497\n",
            "0.1631602793931961\n",
            "0.17458316683769226\n",
            "0.15981538593769073\n",
            "0.17110571265220642\n",
            "0.15656033158302307\n",
            "0.16771209239959717\n",
            "0.1533927023410797\n",
            "0.1644010841846466\n",
            "0.15031032264232635\n",
            "0.1611710786819458\n",
            "0.1473110020160675\n",
            "0.15802079439163208\n",
            "0.1443922519683838\n",
            "0.1549486517906189\n",
            "0.14155225455760956\n",
            "0.15195327997207642\n",
            "0.13878855109214783\n",
            "0.14903265237808228\n",
            "0.13609910011291504\n",
            "0.1461854875087738\n",
            "0.13348183035850525\n",
            "0.1434100866317749\n",
            "0.13093464076519012\n",
            "0.14070439338684082\n",
            "0.12845563888549805\n",
            "0.1380673348903656\n",
            "0.12604275345802307\n",
            "0.13549667596817017\n",
            "0.1236940398812294\n",
            "0.13299113512039185\n",
            "0.12140767276287079\n",
            "0.13054880499839783\n",
            "0.11918189376592636\n",
            "0.1281680464744568\n",
            "0.11701476573944092\n",
            "0.12584728002548218\n",
            "0.11490478366613388\n",
            "0.12358498573303223\n",
            "0.11284992843866348\n",
            "0.12137946486473083\n",
            "0.11084876209497452\n",
            "0.119229257106781\n",
            "0.10889963805675507\n",
            "0.11713278293609619\n",
            "0.10700096935033798\n",
            "0.1150885820388794\n",
            "0.10515125095844269\n",
            "0.11309528350830078\n",
            "0.10334904491901398\n",
            "0.11115124821662903\n",
            "0.10159294307231903\n",
            "0.10925516486167908\n",
            "0.09988141059875488\n",
            "0.1074059009552002\n",
            "0.09821329265832901\n",
            "0.1056017279624939\n",
            "0.0965871810913086\n",
            "0.10384166240692139\n",
            "0.09500183910131454\n",
            "0.10212445259094238\n",
            "0.09345602989196777\n",
            "0.1004488468170166\n",
            "0.0919485092163086\n",
            "0.09881353378295898\n",
            "0.09047827869653702\n",
            "0.0972173810005188\n",
            "0.08904409408569336\n",
            "0.0956595242023468\n",
            "0.08764491230249405\n",
            "0.09413856267929077\n",
            "0.08627963066101074\n",
            "0.09265360236167908\n",
            "0.08494730293750763\n",
            "0.09120360016822815\n",
            "0.08364705741405487\n",
            "0.08978745341300964\n",
            "0.08237776905298233\n",
            "0.08840444684028625\n",
            "0.08113861083984375\n",
            "0.08705353736877441\n",
            "0.07992863655090332\n",
            "0.08573371171951294\n",
            "0.07874703407287598\n",
            "0.08444416522979736\n",
            "0.07759299129247665\n",
            "0.08318406343460083\n",
            "0.07646570354700089\n",
            "0.08195245265960693\n",
            "0.07536420971155167\n",
            "0.08074873685836792\n",
            "0.07428798824548721\n",
            "0.07957202196121216\n",
            "0.07323632389307022\n",
            "0.0784216821193695\n",
            "0.07220835983753204\n",
            "0.07729679346084595\n",
            "0.07120342552661896\n",
            "0.07619664072990417\n",
            "0.07022104412317276\n",
            "0.07512083649635315\n",
            "0.06926040351390839\n",
            "0.07406839728355408\n",
            "0.06832089275121689\n",
            "0.07303890585899353\n",
            "0.06740198284387589\n",
            "0.07203149795532227\n",
            "0.06650304794311523\n",
            "0.07104581594467163\n",
            "0.06562356650829315\n",
            "0.07008111476898193\n",
            "0.06476302444934845\n",
            "0.06913697719573975\n",
            "0.06392078101634979\n",
            "0.06821268796920776\n",
            "0.06309642642736435\n",
            "0.06730777025222778\n",
            "0.06228957325220108\n",
            "0.06642168760299683\n",
            "0.0614994540810585\n",
            "0.06555396318435669\n",
            "0.06072583049535751\n",
            "0.06470423936843872\n",
            "0.05996828153729439\n",
            "0.06387186050415039\n",
            "0.059226177632808685\n",
            "0.06305646896362305\n",
            "0.05849923938512802\n",
            "0.06225752830505371\n",
            "0.057787131518125534\n",
            "0.061474740505218506\n",
            "0.05708928033709526\n",
            "0.06070750951766968\n",
            "0.05640540271997452\n",
            "0.059955596923828125\n",
            "0.055735208094120026\n",
            "0.059218525886535645\n",
            "0.0550781711935997\n",
            "0.05849599838256836\n",
            "0.0544341579079628\n",
            "0.057787537574768066\n",
            "0.05380258709192276\n",
            "0.057092905044555664\n",
            "0.05318336561322212\n",
            "0.05641162395477295\n",
            "0.0525759682059288\n",
            "0.05574345588684082\n",
            "0.05198030546307564\n",
            "0.05508798360824585\n",
            "0.05139594152569771\n",
            "0.05444502830505371\n",
            "0.05082259327173233\n",
            "0.05381417274475098\n",
            "0.050260018557310104\n",
            "0.05319511890411377\n",
            "0.049707986414432526\n",
            "0.05258762836456299\n",
            "0.049166154116392136\n",
            "0.051991403102874756\n",
            "0.04863433912396431\n",
            "0.05140620470046997\n",
            "0.04811225086450577\n",
            "0.050831615924835205\n",
            "0.04759965091943741\n",
            "0.050267577171325684\n",
            "0.04709634929895401\n",
            "0.04971367120742798\n",
            "0.04660205915570259\n",
            "0.049169838428497314\n",
            "0.04611649364233017\n",
            "0.048635661602020264\n",
            "0.045639850199222565\n",
            "0.0481109619140625\n",
            "0.045171357691287994\n",
            "0.0475955605506897\n",
            "0.04471111297607422\n",
            "0.04708927869796753\n",
            "0.044258929789066315\n",
            "0.04659181833267212\n",
            "0.043814562261104584\n",
            "0.04610300064086914\n",
            "0.0433778278529644\n",
            "0.04562258720397949\n",
            "0.04294867441058159\n",
            "0.04515045881271362\n",
            "0.04252667352557182\n",
            "0.04468637704849243\n",
            "0.042111873626708984\n",
            "0.04423022270202637\n",
            "0.04170398786664009\n",
            "0.04378169775009155\n",
            "0.04130296781659126\n",
            "0.04334080219268799\n",
            "0.0409085750579834\n",
            "0.04290717840194702\n",
            "0.040520668029785156\n",
            "0.04248076677322388\n",
            "0.040139101445674896\n",
            "0.04206138849258423\n",
            "0.039763785898685455\n",
            "0.04164886474609375\n",
            "0.0393945686519146\n",
            "0.04124319553375244\n",
            "0.039031267166137695\n",
            "0.04084402322769165\n",
            "0.03867378085851669\n",
            "0.0404512882232666\n",
            "0.03832202032208443\n",
            "0.040064871311187744\n",
            "0.0379757396876812\n",
            "0.03968459367752075\n",
            "0.037634946405887604\n",
            "0.03931039571762085\n",
            "0.037299491465091705\n",
            "0.03894209861755371\n",
            "0.03696923330426216\n",
            "0.03857952356338501\n",
            "0.03664403036236763\n",
            "0.038222670555114746\n",
            "0.03632402420043945\n",
            "0.03787130117416382\n",
            "0.036008693277835846\n",
            "0.03752535581588745\n",
            "0.03569817543029785\n",
            "0.03718477487564087\n",
            "0.03539233282208443\n",
            "0.036849379539489746\n",
            "0.03509121015667915\n",
            "0.03651916980743408\n",
            "0.03479452058672905\n",
            "0.03619372844696045\n",
            "0.034502219408750534\n",
            "0.035873353481292725\n",
            "0.03421416133642197\n",
            "0.035557687282562256\n",
            "0.03393049165606499\n",
            "0.03524667024612427\n",
            "0.033650778234004974\n",
            "0.03494030237197876\n",
            "0.03337521478533745\n",
            "0.03463834524154663\n",
            "0.03310360759496689\n",
            "0.03434097766876221\n",
            "0.032835911959409714\n",
            "0.034047842025756836\n",
            "0.032572031021118164\n",
            "0.03375893831253052\n",
            "0.03231186792254448\n",
            "0.03347426652908325\n",
            "0.03205542638897896\n",
            "0.03319358825683594\n",
            "0.03180255740880966\n",
            "0.032916903495788574\n",
            "0.03155326843261719\n",
            "0.03264427185058594\n",
            "0.031307317316532135\n",
            "0.032375335693359375\n",
            "0.031064892187714577\n",
            "0.03211009502410889\n",
            "0.03082561492919922\n",
            "0.0318487286567688\n",
            "0.030589818954467773\n",
            "0.031590938568115234\n",
            "0.03035717085003853\n",
            "0.03133678436279297\n",
            "0.030127620324492455\n",
            "0.031085968017578125\n",
            "0.02990117110311985\n",
            "0.030838608741760254\n",
            "0.029677677899599075\n",
            "0.03059464693069458\n",
            "0.02945723570883274\n",
            "0.030354082584381104\n",
            "0.029239654541015625\n",
            "0.030116617679595947\n",
            "0.029024982824921608\n",
            "0.029882431030273438\n",
            "0.02881317213177681\n",
            "0.029651224613189697\n",
            "0.02860398218035698\n",
            "0.029423177242279053\n",
            "0.028397608548402786\n",
            "0.029198169708251953\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XcgCqzdi2zo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "737bec39-0ada-45b6-a6ee-7ef9f9ed0c34"
      },
      "cell_type": "code",
      "source": [
        "# Calculate the score after taining\n",
        "with torch.no_grad():\n",
        "  input_ix = prepare_sequence(training_data[0][0],word_to_ix)\n",
        "  target_scores = model(input_ix)\n",
        "  print(target_scores)\n",
        "  \n",
        "# Element i,j of the output is the score for tag (V,NN,DET) j for word i\n",
        "# sentence \"The dog ate the apple\" -> [\"DET\",\"NN\",\"V\",\"DET\",\"NN\"]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-4.2414, -0.8027, -0.6208],\n",
            "        [-5.6365, -0.0145, -4.5287],\n",
            "        [-0.0352, -4.2654, -3.8867],\n",
            "        [-4.5988, -3.9927, -0.0289],\n",
            "        [-5.9115, -0.0199, -4.0771]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KYmdSA-i3KHh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}